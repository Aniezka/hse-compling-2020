{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LSTM, Bidirectional, TimeDistributed, InputLayer\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, Input, concatenate, SpatialDropout1D, Flatten, Conv2D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_filepath = \"negative.csv\"\n",
    "pos_filepath = \"positive.csv\"\n",
    "\n",
    "neg_df = pd.read_csv(neg_filepath, sep=\";\", header=None)\n",
    "pos_df = pd.read_csv(pos_filepath, sep=\";\", header=None)\n",
    "df = pd.concat([neg_df, pos_df])\n",
    "df = df[[3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>на работе был полный пиддес :| и так каждое за...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Коллеги сидят рубятся в Urban terror, а я из-з...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@elina_4post как говорят обещаного три года жд...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Желаю хорошего полёта и удачной посадки,я буду...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Обновил за каким-то лешим surf, теперь не рабо...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   3  4\n",
       "0  на работе был полный пиддес :| и так каждое за... -1\n",
       "1  Коллеги сидят рубятся в Urban terror, а я из-з... -1\n",
       "2  @elina_4post как говорят обещаного три года жд... -1\n",
       "3  Желаю хорошего полёта и удачной посадки,я буду... -1\n",
       "4  Обновил за каким-то лешим surf, теперь не рабо... -1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[4] = df[4].apply(lambda x: 0 if x == -1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [на, работе, был, полный, пиддес, и, так, кажд...\n",
      "1    [коллеги, сидят, рубятся, в, а, я, из, за, дол...\n",
      "2           [как, говорят, обещаного, три, года, ждут]\n",
      "3    [желаю, хорошего, пол, та, и, удачной, посадки...\n",
      "4    [обновил, за, каким, то, лешим, теперь, не, ра...\n",
      "5    [кот, нка, вчера, носик, разбила, плакала, и, ...\n",
      "6    [зашли, а, то, он, опять, затихарился, я, прям...\n",
      "7      [а, вообще, я, не, болею, я, не, выздоравливаю]\n",
      "8    [я, микрофраза, учимся, срать, кирпичами, в, р...\n",
      "9    [я, хочу, с, тобой, помириться, но, сука, я, г...\n",
      "Name: norm, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('[^а-я]', ' ', text)\n",
    "    pred = -1\n",
    "    while pred != len(text):\n",
    "        pred = len(text)\n",
    "        text = text.replace(\"  \", \" \")\n",
    "    return text.lower()\n",
    "\n",
    "def text_to_seq(text):\n",
    "    return [i for i in text.split(\" \") if not i.startswith(\"@\") and 0 < len(i) < 20]\n",
    "\n",
    "\n",
    "docs = []\n",
    "for i in list(df[3].values):\n",
    "    docs.append(text_to_seq(preprocess(i)))\n",
    "\n",
    "df['norm'] = docs\n",
    "print(df['norm'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "for seq in df['norm'].values:\n",
    "    for word in seq:\n",
    "        if word in d:\n",
    "            d[word] += 1\n",
    "        else:\n",
    "            d[word] = 1\n",
    "buf_d = {}\n",
    "for k, v in d.items():\n",
    "    if v > 4:\n",
    "        buf_d[k] = v\n",
    "\n",
    "\n",
    "# word2id = {word:i for i, word in enumerate(set([i for j in df['norm'].values for i in j if i in buf_d.keys()]))}\n",
    "word2id = {word:i + 2 for i, word in enumerate(set([i for j in df['norm'].values for i in j if i in buf_d.keys()]))}\n",
    "word2id['pad'] = 0\n",
    "word2id['unk'] = 1  \n",
    "\n",
    "id2word = {i:word for word, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31961"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2ints(data, smth2id):\n",
    "    int_data = []\n",
    "    for seq in data:\n",
    "        int_seq = []\n",
    "        for i in seq:\n",
    "            if i in smth2id.keys():\n",
    "                int_seq.append(smth2id[i])\n",
    "\n",
    "        int_data.append(int_seq)\n",
    "    return int_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_train, sent_test, y_train_ids, y_test_ids = train_test_split(df['norm'].values,\n",
    "                                                              [[i] for i in df[4].values],\n",
    "                                             \n",
    "                                                                  test_size=0.2,\n",
    "                                                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_ids, X_test_ids, y_train_ids, y_test_ids = train_test_split(data2ints(df['norm'].values, word2id),\n",
    "#                                                               [[i] for i in df[4].values],\n",
    "#                                                               test_size=0.2, random_state=1)\n",
    "\n",
    "X_train_ids, X_test_ids = data2ints(sent_train, word2id), data2ints(sent_test, word2id)\n",
    "# y_train_ids, y_test_ids = data2ints(tag_train, 1), data2ints(tag_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = max([len(i) for i in X_train_ids + X_test_ids])\n",
    "\n",
    "X_train, X_test = pad_sequences(X_train_ids, maxlen=MAX_WORDS, padding='post'), pad_sequences(X_test_ids, maxlen=MAX_WORDS, padding='post')\n",
    "y_train, y_test = pad_sequences(y_train_ids, 1), pad_sequences(y_test_ids, 1)\n",
    "# y_train, y_test = pad_sequences(y_train_ids, maxlяen=sent_max_len, padding='post'), pad_sequences(y_test_ids, maxlen=sent_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = to_categorical(y_train, num_classes=2)\n",
    "# y_test = to_categorical(y_test, num_classes=2)\n",
    "# y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181467, 32) (181467, 1) (45367, 32) (45367, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27372, 23365, 28588, 25518,  9854, 23559, 19272, 19510, 19145,\n",
       "       15076, 31314, 13040,  8639,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.Accuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_9 (Embedding)      (None, 32, 100)           3196100   \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,280,709\n",
      "Trainable params: 3,280,709\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_in = Input(shape=(MAX_WORDS))\n",
    "emb_word = Embedding(input_dim=len(word2id), output_dim=100, mask_zero=True)(word_in)\n",
    "lstm = Bidirectional(LSTM(64, recurrent_dropout=0.2))(emb_word)\n",
    "out = Dense(1, activation=\"sigmoid\")(lstm)\n",
    "\n",
    "model = Model(inputs=word_in, outputs=out)\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181467 samples, validate on 45367 samples\n",
      "Epoch 1/3\n",
      "181467/181467 [==============================] - 80s 441us/sample - loss: 0.5514 - precision: 0.7181 - recall: 0.7198 - accuracy: 0.0000e+00 - val_loss: 0.5151 - val_precision: 0.7379 - val_recall: 0.7585 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "181467/181467 [==============================] - 76s 421us/sample - loss: 0.4621 - precision: 0.7793 - recall: 0.7862 - accuracy: 0.0000e+00 - val_loss: 0.5197 - val_precision: 0.7302 - val_recall: 0.7709 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "181467/181467 [==============================] - 76s 420us/sample - loss: 0.4098 - precision: 0.8147 - recall: 0.8098 - accuracy: 0.0000e+00 - val_loss: 0.5385 - val_precision: 0.7444 - val_recall: 0.7355 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f73d30abc50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=512, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load fasttext vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_file = 'araneum_none_fasttextcbow_300_5_2018.model'\n",
    "model_fasttext = KeyedVectors.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ndarray((len(word2id.keys()), 300))\n",
    "\n",
    "for word in word2id.keys():\n",
    "    weights[word2id[word]] = model_fasttext[word]\n",
    "# [word2id[word]: model[word] for word in }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 32, 300)           9588300   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 9,775,309\n",
      "Trainable params: 187,009\n",
      "Non-trainable params: 9,588,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_in = Input(shape=(MAX_WORDS))\n",
    "emb_word = Embedding(input_dim=len(word2id), output_dim=300, trainable=False, weights=[weights])(word_in)\n",
    "lstm = Bidirectional(LSTM(64, recurrent_dropout=0.2))(emb_word)\n",
    "out = Dense(1, activation=\"sigmoid\")(lstm)\n",
    "\n",
    "model = Model(inputs=word_in, outputs=out)\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181467 samples, validate on 45367 samples\n",
      "Epoch 1/3\n",
      "181467/181467 [==============================] - 62s 340us/sample - loss: 0.6415 - precision: 0.6262 - recall: 0.6647 - accuracy: 0.0000e+00 - val_loss: 0.6152 - val_precision: 0.6490 - val_recall: 0.7119 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "181467/181467 [==============================] - 60s 330us/sample - loss: 0.6081 - precision: 0.6657 - recall: 0.6909 - accuracy: 0.0000e+00 - val_loss: 0.6055 - val_precision: 0.6435 - val_recall: 0.7549 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "181467/181467 [==============================] - 61s 338us/sample - loss: 0.5990 - precision: 0.6722 - recall: 0.6965 - accuracy: 0.0000e+00 - val_loss: 0.5981 - val_precision: 0.6498 - val_recall: 0.7557 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f73ef6bbe90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=512, epoch s=3, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors, FastText\n",
    "\n",
    "ft_model = FastText(size=100)\n",
    "ft_model.build_vocab(sentences=df['norm'].values)\n",
    "ft_model.train(sentences=df['norm'].values, total_examples=len(df['norm'].values), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_train = np.array([ft_model.wv[word] for word in id2word.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 32, 100)           3196100   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,280,709\n",
      "Trainable params: 84,609\n",
      "Non-trainable params: 3,196,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_in = Input(shape=(MAX_WORDS))\n",
    "emb_word = Embedding(input_dim=len(word2id), output_dim=100, trainable=False, weights=[weights_train])(word_in)\n",
    "lstm = Bidirectional(LSTM(64, recurrent_dropout=0.2))(emb_word)\n",
    "out = Dense(1, activation=\"sigmoid\")(lstm)\n",
    "\n",
    "model = Model(inputs=word_in, outputs=out)\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 181467 samples, validate on 45367 samples\n",
      "Epoch 1/3\n",
      "181467/181467 [==============================] - 65s 361us/sample - loss: 0.6743 - precision: 0.5757 - recall: 0.6360 - accuracy: 0.0000e+00 - val_loss: 0.6582 - val_precision: 0.5935 - val_recall: 0.6919 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "181467/181467 [==============================] - 60s 333us/sample - loss: 0.6521 - precision: 0.6116 - recall: 0.6624 - accuracy: 0.0000e+00 - val_loss: 0.6421 - val_precision: 0.6050 - val_recall: 0.7277 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "181467/181467 [==============================] - 61s 335us/sample - loss: 0.6386 - precision: 0.6291 - recall: 0.6646 - accuracy: 0.0000e+00 - val_loss: 0.6386 - val_precision: 0.6021 - val_recall: 0.7815 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f72c0216fd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=512, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat model (learning embedding layer + char embedding layer with CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['д', 'щ', 'с', 'а', 'р', 'й', 'э', 'ю', 'ф', 'г', 'б', 'ш', 'у', 'е', 'к', 'н', 'ц', 'ы', 'о', 'л', 'ч', 'ь', 'з', 'в', 'и', 'х', 'м', 'ж', 'ъ', 'я']\n"
     ]
    }
   ],
   "source": [
    "filtered_vocab = set([word for sent in df['norm'].values for word in sent])\n",
    "\n",
    "chars = set([letter for word in filtered_vocab for letter in word])\n",
    "n_chars = len(chars)\n",
    "print(list(chars)[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id = {c: i + 2 for i, c in enumerate(chars)}\n",
    "char2id[\"pad\"] = 0\n",
    "char2id[\"unk\"] = 1\n",
    "\n",
    "id2char = {i:char for char, i in char2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "максимальная длина слова: 19\n"
     ]
    }
   ],
   "source": [
    "char_max_len = max(len(x) for x in filtered_vocab)\n",
    "print(\"максимальная длина слова:\", char_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2ints(data, smth2id):\n",
    "    int_data = []\n",
    "    for seq in data:\n",
    "        int_seq = []\n",
    "        for i in seq:\n",
    "            int_seq.append(smth2id.get(i.lower(), 1))\n",
    "\n",
    "        int_data.append(int_seq)\n",
    "    return int_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14285, 13787, 4897, 7603, 29415, 11957, 22228]\n",
      "[0]\n",
      "[29415, 13040, 14363, 1537, 14363, 22472, 2604, 31388, 1392]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# X_train_ids, X_test_ids = data2ints(sent_train, word2id), data2ints(sent_test, word2id)\n",
    "# y_train_ids, y_test_ids = data2ints(tag_train, tag2id), data2ints(tag_test, tag2id)\n",
    "\n",
    "\n",
    "print(X_train_ids[0])\n",
    "print(y_train_ids[0])\n",
    "\n",
    "print(X_test_ids[0])\n",
    "print(y_test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_max_len = MAX_WORDS\n",
    "\n",
    "def make_X_char(sentences):\n",
    "    X_char = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sent_seq = []\n",
    "        for i in range(sent_max_len):\n",
    "            word_seq = []\n",
    "            for j in range(char_max_len):\n",
    "                try:\n",
    "                    word_seq.append(char2id[sentence[i][j].lower()])\n",
    "                except:\n",
    "                    word_seq.append(char2id[\"pad\"])\n",
    "            sent_seq.append(word_seq)\n",
    "        X_char.append(np.array(sent_seq))\n",
    "    return np.array(X_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_char_train, X_char_test = make_X_char(sent_train), make_X_char(sent_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181467, 32, 19)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_char_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 32, 19)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 32, 19, 10)   340         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 32)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 32, 17, 12)   372         time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 32, 100)      3196100     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 32, 204)      0           time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 128)          84480       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 128)          137728      time_distributed_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 256)          0           bidirectional_4[0][0]            \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,419,277\n",
      "Trainable params: 3,419,277\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# один вход для слов\n",
    "word_in = Input(shape=(sent_max_len))\n",
    "emb_word = Embedding(input_dim=len(word2id), output_dim=100, mask_zero=True)(word_in)\n",
    "word_lstm = Bidirectional(LSTM(units=64, recurrent_dropout=0.2))(emb_word)\n",
    "\n",
    "# другой вход для символов\n",
    "char_in = Input(shape=(sent_max_len, char_max_len))\n",
    "emb_char = TimeDistributed(Embedding(input_dim=len(char2id), output_dim=10, input_length=char_max_len))(char_in)\n",
    "# свертка на символах (CharRNN) применяется к каждому слову отдельно\n",
    "char_enc = TimeDistributed(Conv1D(filters=12, kernel_size=3))(emb_char)\n",
    "char_flat = TimeDistributed(Flatten())(char_enc)\n",
    "char_lstm = Bidirectional(LSTM(units=64, recurrent_dropout=0.2))(char_flat)\n",
    "\n",
    "# LSTM проходится по всей последовательности, на каждом шаге беря эмбеддинг слова по словарю + символьный эмбеддинг слова \n",
    "x = concatenate([word_lstm, char_lstm])\n",
    "out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=[char_in, word_in], outputs=out)\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "355/355 [==============================] - 109s 277ms/step - loss: 0.5966 - precision: 0.6670 - recall: 0.6984 - accuracy: 0.0000e+00 - val_loss: 0.5141 - val_precision: 0.7413 - val_recall: 0.7601 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "355/355 [==============================] - 100s 282ms/step - loss: 0.4541 - precision: 0.7885 - recall: 0.7927 - accuracy: 0.0000e+00 - val_loss: 0.5129 - val_precision: 0.7442 - val_recall: 0.7590 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "355/355 [==============================] - 101s 284ms/step - loss: 0.3989 - precision: 0.8229 - recall: 0.8187 - accuracy: 0.0000e+00 - val_loss: 0.5344 - val_precision: 0.7535 - val_recall: 0.7203 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f68c81a7c10>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_char_train, X_train],\n",
    "          y_train,\n",
    "          validation_data=([X_char_test, X_test], y_test), batch_size=512, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка датасета, формирование матриц признаков -- 1 балл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведена предобработка текстов и сформированы матрицы признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Внятное сравнение моделей между собой по качеству -- 1 балл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучший результат показывает модель из первого пункта (обучающийся слой векторизации), который показал результат:\n",
    "\n",
    "**val_precision: 0.7444 - val_recall: 0.7355**\n",
    "\n",
    "Остальные модели (предзагруженный fasttext **val_precision: 0.6498 - val_recall: 0.7557** и обученный fasttext **val_precision: 0.6021 - val_recall: 0.7815**) показали результат хуже по **precision**, но зато лучше по **recall**.\n",
    "\n",
    "Сконкатенированная модель (обучающийся слой векторизации по словам + посимвольный вход) показала самый высокий результат среди всех моделей **val_precision: 0.7442 - val_recall: 0.7590**, хоть и переобучилась уже на 3-ей эпохе"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Показано, что качество можно улучшить засчет работы с гиперпараметрами и/или доведения архитектуры (помогло изменение размера слоя/эмбеддингов, добавили Dropout и спаслись от переобучения, добавили слоев и т.д.) -- 1 балл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшим первую модель (обучающийся слой векторизации) путём добавления второго lstm слоя и увеличения юнитов в них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 32, 100)           3196100   \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 32, 256)           234496    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,825,093\n",
      "Trainable params: 3,825,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_in = Input(shape=(MAX_WORDS))\n",
    "emb_word = Embedding(input_dim=len(word2id), output_dim=100, mask_zero=True)(word_in)\n",
    "lstm_1 = Bidirectional(LSTM(128, return_sequences=True, recurrent_dropout=0.2))(emb_word)\n",
    "lstm_2 = Bidirectional(LSTM(128, recurrent_dropout=0.2))(lstm_1)\n",
    "out = Dense(1, activation=\"sigmoid\")(lstm_2)\n",
    "\n",
    "model = Model(inputs=word_in, outputs=out)\n",
    "\n",
    "optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "355/355 [==============================] - 241s 644ms/step - loss: 0.5856 - precision: 0.7080 - recall: 0.7311 - accuracy: 0.0000e+00 - val_loss: 0.5151 - val_precision: 0.7303 - val_recall: 0.7759 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/3\n",
      "355/355 [==============================] - 237s 668ms/step - loss: 0.4450 - precision: 0.7912 - recall: 0.7952 - accuracy: 0.0000e+00 - val_loss: 0.5238 - val_precision: 0.7451 - val_recall: 0.7419 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/3\n",
      "355/355 [==============================] - 238s 670ms/step - loss: 0.3752 - precision: 0.8346 - recall: 0.8330 - accuracy: 0.0000e+00 - val_loss: 0.5504 - val_precision: 0.7384 - val_recall: 0.7443 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f68058fe910>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=512, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На второй эпохе обучения, скор на валидации составил **val_precision: 0.7451 - val_recall: 0.7419**, что выше, чем у базовой модели **val_precision: 0.7444 - val_recall: 0.7355**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
