{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import html2text\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = pd.read_csv(\"competitions_info.csv\")\n",
    "docs = []\n",
    "\n",
    "for doc in data_source.Description.values:\n",
    "    parsed_html = BeautifulSoup(str(doc))\n",
    "    for script in parsed_html([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    docs.append(\" \".join(list(parsed_html.stripped_strings)).replace(\"\\n\", \" \"))\n",
    "data_source[\"prep\"] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data = pd.read_csv(\"competitions_info_summ.csv\")\n",
    "bart_data = pd.read_csv(\"competitions_info_summ_bart.csv\")\n",
    "gpt2_data = pd.read_csv(\"competitions_info_summ_gpt2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(bert_data.Summarized_flag)\n",
    "data[\"Summarized_flag_bart\"] = bart_data.Summarized_flag\n",
    "data[\"Summarized_flag_gpt2\"] = gpt2_data.Summarized_flag\n",
    "data[\"summ_bert\"] = bert_data.Summarized\n",
    "data[\"summ_bart\"] = bart_data.Summarized\n",
    "data[\"summ_gpt2\"] = gpt2_data.Summarized\n",
    "data = data[(data[\"Summarized_flag_bart\"] == True) \n",
    "            & (data[\"Summarized_flag\"] == True)\n",
    "            & (data[\"Summarized_flag_gpt2\"] == True)]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделим краткие описания, которые удачно сгенерировались оба алгоритма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summ = data[data[\"Summarized_flag\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summarized_flag</th>\n",
       "      <th>Summarized_flag_bart</th>\n",
       "      <th>Summarized_flag_gpt2</th>\n",
       "      <th>summ_bert</th>\n",
       "      <th>summ_bart</th>\n",
       "      <th>summ_gpt2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Your success depends upon how closely you can ...</td>\n",
       "      <td>There are 5 essay sets. Each of the sets of es...</td>\n",
       "      <td>Your success depends upon how closely you can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>BACKGROUND AND OBJECTIVES The data set for thi...</td>\n",
       "      <td>Great Candidates of America (GCA) is one of th...</td>\n",
       "      <td>BACKGROUND AND OBJECTIVES The data set for thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>You are presented the opportunity of a lifetim...</td>\n",
       "      <td>The Board of Directors of the National Bureau ...</td>\n",
       "      <td>You are presented the opportunity of a lifetim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>For this challenge, potential Facebook recruit...</td>\n",
       "      <td>Facebook is seeking data-savvy software engine...</td>\n",
       "      <td>For this challenge, potential Facebook recruit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Understanding how and why we are here is one o...</td>\n",
       "      <td>Galaxies come in all shapes, sizes and colors:...</td>\n",
       "      <td>Understanding how and why we are here is one o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Summarized_flag  Summarized_flag_bart  Summarized_flag_gpt2  \\\n",
       "5              True                  True                  True   \n",
       "8              True                  True                  True   \n",
       "12             True                  True                  True   \n",
       "13             True                  True                  True   \n",
       "15             True                  True                  True   \n",
       "\n",
       "                                            summ_bert  \\\n",
       "5   Your success depends upon how closely you can ...   \n",
       "8   BACKGROUND AND OBJECTIVES The data set for thi...   \n",
       "12  You are presented the opportunity of a lifetim...   \n",
       "13  For this challenge, potential Facebook recruit...   \n",
       "15  Understanding how and why we are here is one o...   \n",
       "\n",
       "                                            summ_bart  \\\n",
       "5   There are 5 essay sets. Each of the sets of es...   \n",
       "8   Great Candidates of America (GCA) is one of th...   \n",
       "12  The Board of Directors of the National Bureau ...   \n",
       "13  Facebook is seeking data-savvy software engine...   \n",
       "15  Galaxies come in all shapes, sizes and colors:...   \n",
       "\n",
       "                                            summ_gpt2  \n",
       "5   Your success depends upon how closely you can ...  \n",
       "8   BACKGROUND AND OBJECTIVES The data set for thi...  \n",
       "12  You are presented the opportunity of a lifetim...  \n",
       "13  For this challenge, potential Facebook recruit...  \n",
       "15  Understanding how and why we are here is one o...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_summ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 5\n",
    "texts_5_10 = []\n",
    "texts_10_20 = []\n",
    "texts_20_30 = []\n",
    "\n",
    "for ind, data_index in enumerate(data_summ.index):\n",
    "    source_text = data_source.loc[[data_index]].prep.values[0]\n",
    "    # Пропустим НЕ английские тексты\n",
    "    if detect(source_text) != 'en':\n",
    "        continue\n",
    "    row = data_summ.loc[data_index]\n",
    "    sent_num = len([x for x in source_text.split(\".\") if len(x) > 5])\n",
    "    data = f\"Text id:\\n{data_index}\\nSource text:\\n{source_text}\\n\\n\" \\\n",
    "    f\"BERT summarization:\\n{row[3]}\\n\\nBART summarization:\\n{row[4]}\\n\\nGPT2 summarization:\\n{row[5]}\\n\\n\" \\\n",
    "    + (\"-\"*100)\n",
    "    if 5 < sent_num <= 10 and not source_text in [text[0] for text in texts_5_10]:\n",
    "        texts_5_10.append((source_text, data))\n",
    "    elif 10 < sent_num <= 20 and not source_text in [text[0] for text in texts_10_20]:\n",
    "        texts_10_20.append((source_text, data))\n",
    "    elif 20 < sent_num <= 30 and not source_text in [text[0] for text in texts_20_30]:\n",
    "        texts_20_30.append((source_text, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценим суммаризацию на текстах разной длины:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5-10 предложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text id:\n",
      "35\n",
      "Source text:\n",
      "RNA polymerase II is crucial for gene transcription (DNA to RNA) and is therefore the most studied polymerase. Transcription start sites (TSS), where the polymerase II first binds to the DNA, are  marked by specific short DNA sequences. Also other structural properties of the genome (histons, nucleosomes) need to allow binding. After polymerase II binds to the DNA, it opens the DNA helix and moves along the DNA during transcription. The column polII_presence  in data specifies experimentally measured concentration of polymerase II at each position in the genome. Each line in data specifies a position (nucleotide) in the DNA sequence. Nucleotides are listed in the same order as the appear in the genome and are described with: polymerase II presence (polII_presence - target variable, absent in testing file): 4 (very present), 3 (present), 2 (insignificant), 1 (absent), 0 (absent with very high probability) nucleotide (DNA): A, T, C, G, gene structure on + strand (gene_structure_on_plus), gene structure on - strand (gene_structure_on_minus): _ (intergenomic region), O (ORF), I (intron), E (exon that is not ORF), untranslated ORFs and transcription start sites (UTR&TSS): 3utr (untranslated tail), 5utr (untranslated head), tss (transcription start), _ (the rest), histone binding: H (binding), _ (no binding), histone modifications: PP (very significant change), P (change), Z (insignificant), N (no change), NN (no change with very high probability), nucleosome_occupancy: P (significant occupancy), Z (insignificant), N (significantly low occupancy), expression of genes or region (expression): P (very high expression), Z (insignificant), N (very low expression). Submit your predictions of the presence of polymerase II for the testing set as a single number per line (keep the same order).\n",
      "\n",
      "BERT summarization:\n",
      "RNA polymerase II is crucial for gene transcription (DNA to RNA) and is therefore the most studied polymerase. The column polII_presence  in data specifies experimentally measured concentration of polymerase II at each position in the genome.\n",
      "\n",
      "BART summarization:\n",
      "RNA polymerase II is crucial for gene transcription (DNA to RNA) and is therefore the most studied polymerase. Transcription start sites (TSS) are  marked by specific short DNA sequences. After polymer enzyme II binds to the DNA, it opens the DNA helix and moves along the DNA during transcription.\n",
      "\n",
      "GPT2 summarization:\n",
      "RNA polymerase II is crucial for gene transcription (DNA to RNA) and is therefore the most studied polymerase. Transcription start sites (TSS), where the polymerase II first binds to the DNA, are  marked by specific short DNA sequences.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "101\n",
      "Source text:\n",
      "(right whale illustration courtesy of Pieter Folkens, ©2011) This competition complements the previously held Marinexplore Whale Detection Challenge , in which Cornell University provided data from a ship monitoring application termed \"Auto Buoy\", or AB Monitoring System. In the Marinexplore challenge we received solutions that exceeded 98% accuracy and will ultimately advance the process of automatically classifying North Atlantic Right Whales using the AB Monitoring Platform. Since the results from the previous challenge proved so successful, we decided to extend the goals and consider applications that involve running algorithms on archival data recorded using portable hydrophone assemblies, otherwise referred to as Marine Autonomous Recording Unit (or MARU’s). Since Cornell and its partners have been using the MARU for over a decade, a sizable collection of data has been accumulated. This data spans several ocean basins and covers a variety of marine mammal species. Solutions to this challenge will be ported to a High Performance Computing (HPC) platform, being developed in part through funding provided by the Office of Naval Research (ONR grant N000141210585, Dugan, Clark, LeCun and Van Parijs). Together, Cornell will combine algorithms, HPC technologies and its data archives to explore data using highly accurate measuring tools. We encourage participants who developed prior solutions (through the collaboration with Marinexplore) to test them on this data. The results will be presented at the Workshop on Machine Learning for Bioacoustics at ICML 2013.\n",
      "\n",
      "BERT summarization:\n",
      "(right whale illustration courtesy of Pieter Folkens, ©2011) This competition complements the previously held Marinexplore Whale Detection Challenge , in which Cornell University provided data from a ship monitoring application termed \"Auto Buoy\", or AB Monitoring System. In the Marinexplore challenge we received solutions that exceeded 98% accuracy and will ultimately advance the process of automatically classifying North Atlantic Right Whales using the AB Monitoring Platform.\n",
      "\n",
      "BART summarization:\n",
      "This competition complements the previously held Marinexplore Whale Detection Challenge. Cornell will combine algorithms, HPC technologies and its data archives to explore data using highly accurate measuring tools. The results will be presented at the Workshop on Machine Learning for Bioacoustics at ICML 2013.\n",
      "\n",
      "GPT2 summarization:\n",
      "(right whale illustration courtesy of Pieter Folkens, ©2011) This competition complements the previously held Marinexplore Whale Detection Challenge , in which Cornell University provided data from a ship monitoring application termed \"Auto Buoy\", or AB Monitoring System. In the Marinexplore challenge we received solutions that exceeded 98% accuracy and will ultimately advance the process of automatically classifying North Atlantic Right Whales using the AB Monitoring Platform.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "345\n",
      "Source text:\n",
      "In their first recruiting competition, Telstra is challenging Kagglers to predict the severity of service disruptions on their network. Using a dataset of features from their service logs, you're tasked with predicting if a disruption is a momentary glitch or a total interruption of connectivity. Telstra is on a journey to enhance the customer experience - ensuring everyone in the company is putting customers first. In terms of its expansive network, this means continuously advancing how it predicts the scope and timing of service disruptions. Telstra wants to see how you would help it drive customer advocacy by developing a more advanced predictive model for service disruptions and to help it better serve its customers. This challenge was crafted as a simulation of the type of problem you might tackle as a member of the team at Telstra. Kagglers who stand out will be considered for data science roles in Telstra's Big Data team in Telstra’s absolute discretion. Highly-ranked participants will combine technical expertise and intuition in data science problems with a keen business sense and an effortless ability to work with technical and non-technical staff to turn data into real changes that impact customers. Highly-ranked participants will be considered by Telstra for interviews for employment, based on their work in the Competition and ability to meet the selection criteria for any suitable open job vacancy in Melbourne and Sydney, Australia. Participation in this Competition is not a recruitment process and Kaggle does not provide Telstra with recruitment services.\n",
      "\n",
      "BERT summarization:\n",
      "In their first recruiting competition, Telstra is challenging Kagglers to predict the severity of service disruptions on their network. Using a dataset of features from their service logs, you're tasked with predicting if a disruption is a momentary glitch or a total interruption of connectivity. Telstra is on a journey to enhance the customer experience - ensuring everyone in the company is putting customers first.\n",
      "\n",
      "BART summarization:\n",
      "Telstra is on a journey to enhance the customer experience. In terms of its expansive network, this means continuously advancing how it predicts the scope and timing of service disruptions. Kagglers who stand out will be considered for data science roles in Telstra's Big Data team.\n",
      "\n",
      "GPT2 summarization:\n",
      "In their first recruiting competition, Telstra is challenging Kagglers to predict the severity of service disruptions on their network. Using a dataset of features from their service logs, you're tasked with predicting if a disruption is a momentary glitch or a total interruption of connectivity. Kagglers who stand out will be considered for data science roles in Telstra's Big Data team in Telstra’s absolute discretion.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "1120\n",
      "Source text:\n",
      "Where is a social network user located? That question is explored in this Kaggle in-class competition for COMP90051. Given training data on user locations, post counts, post times (UTC) and the social graph between users, your goal is to predict the latitude and longitude coordinate-pairs for a test set of users. To succeed in this competition you will have to ask yourself: What machine learning task is best suited to this problem? What software should I use? How should I load in all the data, and preprocess it into features? What learning algorithms might be suitable? How do I tune parameters to the learners so that test performance is optimised (without overfitting to the training data, or leaderboard portion of the test data)? How can I tune my approach using just the training data - effectively giving me more ability to fine-tune? The aim of this competition is to make your first COMP90051 project fun! A little competition will encourage the teams of 2 to really explore what is possible with this real dataset. Grading for the project includes a small component being the rank achieved in the competition and the absolute score. Our hope is for this to encourage a little healthy competition (and not too much sharing between teams). However a substantial part of the mark will come from raw performance, and a small writeup - your grade is not just determined by others! We hope you have lots of fun! Further details of the problem and datasets, updates and discussion may be located in LMS in addition to this Kaggle competition site.\n",
      "\n",
      "BERT summarization:\n",
      "That question is explored in this Kaggle in-class competition for COMP90051. Given training data on user locations, post counts, post times (UTC) and the social graph between users, your goal is to predict the latitude and longitude coordinate-pairs for a test set of users. Our hope is for this to encourage a little healthy competition (and not too much sharing between teams).\n",
      "\n",
      "BART summarization:\n",
      "Kaggle in-class competition for COMP90051. Given training data on user locations, post counts, post times (UTC) and the social graph between users. Your goal is to predict the latitude and longitude coordinate-pairs for a test set of users.\n",
      "\n",
      "GPT2 summarization:\n",
      "That question is explored in this Kaggle in-class competition for COMP90051. How do I tune parameters to the learners so that test performance is optimised (without overfitting to the training data, or leaderboard portion of the test data)? However a substantial part of the mark will come from raw performance, and a small writeup - your grade is not just determined by others!\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "1288\n",
      "Source text:\n",
      "Given a training set of past churn data, your goal in this homework is to predict whether a person will leave the network (churn) or stay. Steps: 1. Train and build a model based using the training data: any model studies in class is acceptable, such as a decision tree, logistic regression, or support vector machine. 2. Use your model to predict the outputs for the test data. 3. Upload your prediction as a file to the kaggle competition to be evaluated and ranked. Data Description: Each row represents a customer of the network, with the parameters for each customer described below. The data consists of 20,000 customers, split into 90% (18,000) for training data and the remaining 10% (2,000) as test data (holdout). You can find the labeled training data in 'train.csv' and unlabeled test data in 'test.csv'. Features of each customer : COLLEGE                        Is the customer college educated? INCOME                         Annual income OVERAGE                        Average overcharges per month LEFTOVER                       Average number of leftover minutes per month HOUSE                          Estimated value of dwelling (from census tract) HANDSET_PRICE                  Cost of phone OVER_15MINS_CALLS_PER_MONTH    Average number of long calls (15 mins or over) per month AVERAGE_CALL_DURATION          Average duration of a call REPORTED_SATISFACTION          Reported level of satisfaction REPORTED_USAGE_LEVEL           Self-reported usage level CONSIDERING_CHANGE_OF_PLAN     Whether the customer considered changing their plan LEAVE (Target variable)        Did the customer stay or leave (churn)?\n",
      "\n",
      "BERT summarization:\n",
      "Given a training set of past churn data, your goal in this homework is to predict whether a person will leave the network (churn) or stay. The data consists of 20,000 customers, split into 90% (18,000) for training data and the remaining 10% (2,000) as test data (holdout).\n",
      "\n",
      "BART summarization:\n",
      "Given a training set of past churn data, your goal in this homework is to predict whether a person will leave the network (churn) or stay. The data consists of 20,000 customers, split into 90% for training data and the remaining 10% (2,000) as test data (holdout)\n",
      "\n",
      "GPT2 summarization:\n",
      "Given a training set of past churn data, your goal in this homework is to predict whether a person will leave the network (churn) or stay. You can find the labeled training data in 'train.csv' and unlabeled test data in 'test.csv'.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "1635\n",
      "Source text:\n",
      "The great majority of languages written in Latin script use either diacritics or extended Latin characters to more accurately represent the sounds in the language.  This can present a problem when speakers try to type the language on the computer or on a mobile device, either because they lack a suitable keyboard to input the correct characters, or doing so is sufficiently cumbersome that it's easier to type an \"ASCII approximation\".  The problem of automatic diacritic restoration (or more generally what I've called \"Unicodification\") becomes an important pre-processing step for many natural language processing applications like Machine Translation, Text-to-Speech, etc. This competition focuses on unicodification of the Azerbaijani language.  Azerbaijani is an interesting case for a few reasons: Characters with diacritics (ç, ,ğ, ö, ş, ü) and extended Latin characters (ı, ə) are quite common in running text; typically more than half of the words in a given text will contain at least one such special character As a Turkic language, it is morphologically quite complex which means that word-based models alone aren't likely to give optimal results It has vowel harmony, and therefore some interesting long-distance dependencies when restoring diacritics It is an under-resourced language in comparison with major European languages like French, English, and German.  For example, I know of no open source morphological analyzer for Azerbaijani. It nevertheless has a reasonably strong web presence so it's easy enough to find correctly-encoded text for training.\n",
      "\n",
      "BERT summarization:\n",
      "The great majority of languages written in Latin script use either diacritics or extended Latin characters to more accurately represent the sounds in the language. The problem of automatic diacritic restoration (or more generally what I've called \"Unicodification\") becomes an important pre-processing step for many natural language processing applications like Machine Translation, Text-to-Speech, etc.\n",
      "\n",
      "BART summarization:\n",
      "The great majority of languages written in Latin script use either diacritics or extended Latin characters to more accurately represent the sounds in the language. This can present a problem when speakers try to type the language on the computer or on a mobile device. The problem of automatic diacritic restoration becomes an important pre-processing step for many natural language processing applications like Machine Translation, Text-to-Speech, etc.\n",
      "\n",
      "GPT2 summarization:\n",
      "The great majority of languages written in Latin script use either diacritics or extended Latin characters to more accurately represent the sounds in the language. For example, I know of no open source morphological analyzer for Azerbaijani.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "2168\n",
      "Source text:\n",
      "Who do you think hates traffic more - humans or self-driving cars? The position of nearby automobiles is a key question for autonomous vehicles ― and it's at the heart of our newest challenge.   Self-driving cars have come a long way in recent years, but they're still not flawless. Consumers and lawmakers remain wary of adoption, in part because of doubts about vehicles’ ability to accurately perceive objects in traffic.  Baidu's Robotics and Autonomous Driving Lab (RAL), along with Peking University, hopes to close the gap once and for all with this challenge. They’re providing Kagglers with more than 60,000 labeled 3D car instances from 5,277 real-world images, based on industry-grade CAD car models.  Your challenge: develop an algorithm to estimate the absolute pose of vehicles (6 degrees of freedom) from a single image in a real-world traffic environment.  Succeed and you'll help improve computer vision. That, in turn, will bring autonomous vehicles a big step closer to widespread adoption, so they can help reduce the environmental impact of our growing societies.   Please cite the following paper when using the dataset: ApolloCar3D: A Large 3D Car Instance Understanding Benchmark for Autonomous Driving @inproceedings{song2019apollocar3d,   title={Apollocar3d: A large 3d car instance understanding benchmark for autonomous driving},   author={Song, Xibin and Wang, Peng and Zhou, Dingfu and Zhu, Rui and Guan, Chenye and Dai, Yuchao and Su, Hao and Li, Hongdong and Yang, Ruigang},   booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},   pages={5452--5462},   year={2019} }\n",
      "\n",
      "BERT summarization:\n",
      "Who do you think hates traffic more - humans or self-driving cars? Self-driving cars have come a long way in recent years, but they're still not flawless. Baidu's Robotics and Autonomous Driving Lab (RAL), along with Peking University, hopes to close the gap once and for all with this challenge.\n",
      "\n",
      "BART summarization:\n",
      "The position of nearby automobiles is a key question for autonomous vehicles. Baidu's Robotics and Autonomous Driving Lab (RAL) hopes to close the gap once and for all with this challenge. They’re providing Kagglers with more than 60,000 labeled 3D car instances from 5,277 real-world images.\n",
      "\n",
      "GPT2 summarization:\n",
      "Who do you think hates traffic more - humans or self-driving cars? Consumers and lawmakers remain wary of adoption, in part because of doubts about vehicles’ ability to accurately perceive objects in traffic.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "3015\n",
      "Source text:\n",
      "**File descriptions**   - kaggle_train.csv - the training set  - kaggle_test.csv - the test set  - kaggle_codebook.txt - full description of each column  - kaggle_sample_submission.csv - sample submission file  **Data fields**  Here's a brief version of what you'll find in the codebook  file.   - SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.  - MSSubClass: The building class  - MSZoning: The general zoning classification  - LotFrontage: Linear feet of street connected to property  - LotArea: Lot size in square feet  - Street: Type of road access  - Alley: Type of alley access  - LotShape: General shape of property  - LandContour: Flatness of the property  - Utilities: Type of utilities available  - LotConfig: Lot configuration  - LandSlope: Slope of property  - Neighborhood: Physical locations within Ames city limits  - Condition1: Proximity to main road or railroad  - Condition2: Proximity to main road or railroad (if a second is present)  - BldgType: Type of dwelling  - HouseStyle: Style of dwelling  - OverallQual: Overall material and finish quality  - OverallCond: Overall condition rating  - YearBuilt: Original construction date  - YearRemodAdd: Remodel date  - RoofStyle: Type of roof  - RoofMatl: Roof material  - Exterior1st: Exterior covering on house  - Exterior2nd: Exterior covering on house (if more than one material)  - MasVnrType: Masonry veneer type  - MasVnrArea: Masonry veneer area in square feet  - ExterQual: Exterior material quality  - ExterCond: Present condition of the material on the exterior  - Foundation: Type of foundation  - BsmtQual: Height of the basement  - BsmtCond: General condition of the basement  - BsmtExposure: Walkout or garden level basement walls  - BsmtFinType1: Quality of basement finished area  - BsmtFinSF1: Type 1 finished square feet  - BsmtFinType2: Quality of second finished area (if present)  - BsmtFinSF2: Type 2 finished square feet  - BsmtUnfSF: Unfinished square feet of basement area  - TotalBsmtSF: Total square feet of basement area  - Heating: Type of heating  - HeatingQC: Heating quality and condition  - CentralAir: Central air conditioning  - Electrical: Electrical system  - 1stFlrSF: First Floor square feet  - 2ndFlrSF: Second floor square feet  - LowQualFinSF: Low quality finished square feet (all floors)  - GrLivArea: Above grade (ground) living area square feet  - BsmtFullBath: Basement full bathrooms  - BsmtHalfBath: Basement half bathrooms  - FullBath: Full bathrooms above grade  - HalfBath: Half baths above grade  - Bedroom: Number of bedrooms above basement level  - Kitchen: Number of kitchens  - KitchenQual: Kitchen quality  - TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)  - Functional: Home functionality rating  - Fireplaces: Number of fireplaces  - FireplaceQu: Fireplace quality  - GarageType: Garage location  - GarageYrBlt: Year garage was built  - GarageFinish: Interior finish of the garage  - GarageCars: Size of garage in car capacity  - GarageArea: Size of garage in square feet  - GarageQual: Garage quality  - GarageCond: Garage condition  - PavedDrive: Paved driveway  - WoodDeckSF: Wood deck area in square feet  - OpenPorchSF: Open porch area in square feet  - EnclosedPorch: Enclosed porch area in square feet  - 3SsnPorch: Three season porch area in square feet  - ScreenPorch: Screen porch area in square feet  - PoolArea: Pool area in square feet  - PoolQC: Pool quality  - Fence: Fence quality  - MiscFeature: Miscellaneous feature not covered in other categories  - MiscVal: $Value of miscellaneous feature  - MoSold: Month Sold  - YrSold: Year Sold  - SaleType: Type of sale  - SaleCondition: Condition of sale\n",
      "\n",
      "BERT summarization:\n",
      "**File descriptions**   - kaggle_train.csv - the training set  - kaggle_test.csv - the test set  - kaggle_codebook.txt - full description of each column  - kaggle_sample_submission.csv - sample submission file  **Data fields**  Here's a brief version of what you'll find in the codebook  file. This is the target variable that you're trying to predict.\n",
      "\n",
      "BART summarization:\n",
      " kaggle_train.csv is the training set. kaggling.com's codebook is the test set. The codebook contains the data you'll find in the train and test files. The data in the codebook file includes the sale price and other data fields.\n",
      "\n",
      "GPT2 summarization:\n",
      "**File descriptions**   - kaggle_train.csv - the training set  - kaggle_test.csv - the test set  - kaggle_codebook.txt - full description of each column  - kaggle_sample_submission.csv - sample submission file  **Data fields**  Here's a brief version of what you'll find in the codebook  file.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "3300\n",
      "Source text:\n",
      "UTIVA COHORT 1 BIG DATA WITH PYTHON INCLASS COMPETITION This competition serves the purpose of the regular Hackathon organized by Utiva after every program, so the purpose of the competition is to help students test their knowledge of the concepts taught in class and also used to determine the overall best performing student in the cohort. CASE STUDY : EMPLOYEE ATTRITION CONTROL You have been provided with a fictitious data of a company X that is currently having challenge of controlling attrition. The data contains information about \"Existing employees\" and \"Employees who have left\". You have been called in by the company as a skilled Data Scientist  to use your machine learning skills to study the pattern of attrition. With this insight, you can determine the important features among the available features that can be used to predict which employee will leave next or not.  The dataset contains these variables as explained below: EmployeeID : System generated Unique ID Satisfaction Level : Employee satisfaction level on a scale of 0-1 Last evaluation : The last evaluation rate of the employee on a scale of 0-1 Number of projects : Number of projects the employee is working on or worked on Average monthly hours : The number of hours the employee work   YearofRecruitment : The year that each staff was recruited into the company Work Accident: An indicator whether they have had a work accident. If Yes, it is a 1 and if No it is a 0. Promoted: An indicator whether they have had a promotion in the last 5 years. If Yes, it is a 1 and if No it is a 0. Departments : Operational department where each employee works Salary : How much each employee earn\n",
      "\n",
      "BERT summarization:\n",
      "UTIVA COHORT 1 BIG DATA WITH PYTHON INCLASS COMPETITION This competition serves the purpose of the regular Hackathon organized by Utiva after every program, so the purpose of the competition is to help students test their knowledge of the concepts taught in class and also used to determine the overall best performing student in the cohort.\n",
      "\n",
      "BART summarization:\n",
      "This competition serves the purpose of the regular Hackathon organized by Utiva after every program. It is to help students test their knowledge of the concepts taught in class and also used to determine the overall best performing student in the cohort. You have been provided with a fictitious data of a company X that is currently having challenge of controlling attrition. The data contains information about \"Existing employees\" and \"Employees who have left\"\n",
      "\n",
      "GPT2 summarization:\n",
      "UTIVA COHORT 1 BIG DATA WITH PYTHON INCLASS COMPETITION This competition serves the purpose of the regular Hackathon organized by Utiva after every program, so the purpose of the competition is to help students test their knowledge of the concepts taught in class and also used to determine the overall best performing student in the cohort. You have been called in by the company as a skilled Data Scientist  to use your machine learning skills to study the pattern of attrition.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "3301\n",
      "Source text:\n",
      "UTIVA AI SCHOOL COHORT INCLASS COMPETITION This competition serves the purpose of the regular Hackathon organized by Utiva after every program, so the purpose of the competition is to help students test their knowledge of the concepts taught in class and also used to determine the overall best performing student in the cohort. CASE STUDY : EMPLOYEE ATTRITION CONTROL You have been provided with a fictitious data of a company X that is currently having challenge of controlling attrition. The data contains information about \"Existing employees\" and \"Employees who have left\". You have been called in by the company as a skilled Data Scientist  to use your machine learning skills to study the pattern of attrition. With this insight, you can determine the important features among the available features that can be used to predict which employee will leave next or not.  The dataset contains these variables as explained below: EmployeeID : System generated Unique ID Satisfaction Level : Employee satisfaction level on a scale of 0-1 Last evaluation : The last evaluation rate of the employee on a scale of 0-1 Number of projects : Number of projects the employee is working on or worked on Average monthly hours : The number of hours the employee work   YearofRecruitment : The year that each staff was recruited into the company Work Accident: An indicator whether they have had a work accident. If Yes, it is a 1 and if No it is a 0. Promoted: An indicator whether they have had a promotion in the last 5 years. If Yes, it is a 1 and if No it is a 0. Departments : Operational department where each employee works Salary : How much each employee earn\n",
      "\n",
      "BERT summarization:\n",
      "UTIVA AI SCHOOL COHORT INCLASS COMPETITION This competition serves the purpose of the regular Hackathon organized by Utiva after every program, so the purpose of the competition is to help students test their knowledge of the concepts taught in class and also used to determine the overall best performing student in the cohort.\n",
      "\n",
      "BART summarization:\n",
      "This competition serves the purpose of the regular Hackathon organized by Utiva after every program. It is to help students test their knowledge of the concepts taught in class and also used to determine the overall best performing student in the cohort. You have been provided with a fictitious data of a company X that is currently having challenge of controlling attrition. The data contains information about \"Existing employees\" and \"Employees who have left\"\n",
      "\n",
      "GPT2 summarization:\n",
      "UTIVA AI SCHOOL COHORT INCLASS COMPETITION This competition serves the purpose of the regular Hackathon organized by Utiva after every program, so the purpose of the competition is to help students test their knowledge of the concepts taught in class and also used to determine the overall best performing student in the cohort. You have been called in by the company as a skilled Data Scientist  to use your machine learning skills to study the pattern of attrition.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "3539\n",
      "Source text:\n",
      "CASE STUDY: SOMEFUN GROUP OF COMPANIES - ALGORITHMIC STAFF PROMOTION Mike Somefun, 38, is the heir apparent to the highly revered Mike business dynasty. The enterprise has spanned decades with vast investment interest in all the various sectors of the economy.  Mike has worked for 16 years in Europe and America after his first and second degrees at Harvard University where he studied Accounting. He is a very experienced technocrat and a global business leader who rose through the rank to become a Senior Vice President at a leading US business conglomerate. His dad is now 70 and has invited him to take over the company with a mandate to take it to the next level of growth as a sustainable legacy. Mike is trusted by his father and his siblings to lead this mandate.  On resumption, he noticed some irregularities in the system and part of these irregularities is credit card fraud detection which has has always been reported for a while now.  You have been called in by Mike to use your machine learning skills to study the pattern of Fraud. With this insight, he can understand the important features among available features that can be used to build a system that will detect fraud automatically. FILE DESCRIPTION Data fields For training file Merchant_id - an anonymous id unique to a given customer Average Amount/transaction/day - Average amount of transaction per day Transaction_amount -  Amount of transaction Is declined - If the transaction is declined or not (0= No, 1= Yes) Total Number of declines/day - Total Number of declines per day isForeignTransaction -Is the transactionForeign? (0= No, 1= Yes) isHighRiskCountry - Is the country a High Risk Country?  (0= No, 1= Yes) Daily_chargeback_avg_amt - Daily amount of average charges 6_month_avg_chbk_amt - Average of 6_month charges 6-month_chbk_freq - frequency of 6 month charges isFradulent - If transaction is fraudulent or not (THIS IS THE TARGET VARIABLE), (0= No, 1= Yes For Test file Merchant_id - an anonymous id unique to a given customer Average Amount/transaction/day - Average amount of transaction per day Transaction_amount -  Amount of transaction Is declined - If the transaction is declined or not (0= No, 1= Yes) Total Number of declines/day - Total Number of declines per day isForeignTransaction -Is the transactionForeign? (0= No, 1= Yes) isHighRiskCountry - Is the country a High Risk Country?  (0= No, 1= Yes) Daily_chargeback_avg_amt - Daily amount of average charges 6_month_avg_chbk_amt - Average of 6_month charges 6-month_chbk_freq - frequency of 6 month charges\n",
      "\n",
      "BERT summarization:\n",
      "CASE STUDY: SOMEFUN GROUP OF COMPANIES - ALGORITHMIC STAFF PROMOTION Mike Somefun, 38, is the heir apparent to the highly revered Mike business dynasty. The enterprise has spanned decades with vast investment interest in all the various sectors of the economy. His dad is now 70 and has invited him to take over the company with a mandate to take it to the next level of growth as a sustainable legacy.\n",
      "\n",
      "BART summarization:\n",
      "Mike Somefun, 38, is the heir apparent to the highly revered Mike business dynasty. His dad is now 70 and has invited him to take over the company with a mandate to take it to the next level of growth as a sustainable legacy. Mike has worked for 16 years in Europe and America after his first and second degrees at Harvard University where he studied Accounting.\n",
      "\n",
      "GPT2 summarization:\n",
      "CASE STUDY: SOMEFUN GROUP OF COMPANIES - ALGORITHMIC STAFF PROMOTION Mike Somefun, 38, is the heir apparent to the highly revered Mike business dynasty. His dad is now 70 and has invited him to take over the company with a mandate to take it to the next level of growth as a sustainable legacy. 0= No, 1= Yes) isHighRiskCountry - Is the country a High Risk Country?\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "3887\n",
      "Source text:\n",
      "Shared bicycle systems have expanded all over the world and every day it is more and more common to find them in any city. ![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F258363%2Fcd8b56d413b7e0ece92cd2757e391195%2Fbikes3.png?generation=1572553770849131&alt=media) Every day, thousands of citizens use these bicycles as a means of transportation, but the decision to use a bicycle at a certain time can be influenced by many factors: will it rain? is it too hot? is it time to enter work? ... and many more. The city council of this city has requested to the company that manages the system of shared bicycles a forecast of the demand of bicycles every hour, for the next 31 days. The bike sharing company has a database with historical data on the hourly demand for bicycles and weather forecasting from 2017-01-01 00:00:00 to 2019-08-26 23:00:00 and is promoting a datathon with these data to hire the data-scientist which better result in the challenge. The datathon works in the following way: they provide as training set the hourly demand for bicycles and the hourly weather forecast from 2017-01-01 00:00:00 to 2019-07-26 23:00:00, and as a test set, they provide only the hourly weather forecast from 2019-07-27 00:00:00 to 2019-08-26 23:00:00, and they are asking us to predict the hourly bicycle demand for this period They are also providing as a separate file the holidays from the whole period, 2017-01-01 to 2019-08-26 The shared bicycle company challenges us to predict the hourly demand for bicycles. Let's go for it!\n",
      "\n",
      "BERT summarization:\n",
      "Shared bicycle systems have expanded all over the world and every day it is more and more common to find them in any city. ![]( The datathon works in the following way: they provide as training set the hourly demand for bicycles and the hourly weather forecast from 2017-01-01 00:00:00 to 2019-07-26 23:00:00, and as a test set, they provide only the hourly weather forecast from 2019-07-27 00:00:00 to 2019-08-26 23:00:00, and they are asking us to predict the hourly bicycle demand for this period They are also providing as a separate file the holidays from the whole period, 2017-01-01 to 2019-08-26 The shared bicycle company challenges us to predict the hourly demand for bicycles.\n",
      "\n",
      "BART summarization:\n",
      "Shared bicycle systems have expanded all over the world and every day it is more and more common to find them in any city. City council of this city has requested to the company that manages the system of shared bicycles a forecast of the demand of bicycles every hour, for the next 31 days. The shared bicycle company challenges us to predict the hourly demand for bicycles.\n",
      "\n",
      "GPT2 summarization:\n",
      "Shared bicycle systems have expanded all over the world and every day it is more and more common to find them in any city. ![]( The bike sharing company has a database with historical data on the hourly demand for bicycles and weather forecasting from 2017-01-01 00:00:00 to 2019-08-26 23:00:00 and is promoting a datathon with these data to hire the data-scientist which better result in the challenge.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "4102\n",
      "Source text:\n",
      "Folk Developers Community presents DataScience Hackathon Problem Statement: A US bike-sharing provider `XBikes` has a daily dataset on the rental bikes based on various environmental and seasonal settings. It wishes to use this data to understand the factors affecting the demand for these shared bikes in the American market and come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown due to corona pandemic comes to an end. Essentially, the company wants to know — - Which variables are significant in predicting the demand for shared bikes. - How well those variables describe the bike demands - Predict the count of Bikes used. Essentially, the company wants to know — Which variables are significant in predicting the demand for shared bikes. How well those variables describe the bike demands So interpretation is important! Winners will be decided based on the analysis done in the Jupyter notebook and R2 score, So please submit your notebooks Acknowledgements we are hosting this competition for the machine learning community to use for fun and practice. This dataset was provided by Hadi Fanaee Tork using data from Capital Bikeshare. We also thank the UCI machine learning repository for hosting the dataset. If you use the problem in publication, please cite:  Fanaee-T, Hadi, and Gama, Joao, Event labeling combining ensemble detectors and background knowledge, Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg.\n",
      "\n",
      "BERT summarization:\n",
      "Folk Developers Community presents DataScience Hackathon Problem Statement: A US bike-sharing provider `XBikes` has a daily dataset on the rental bikes based on various environmental and seasonal settings. Essentially, the company wants to know — Which variables are significant in predicting the demand for shared bikes. Winners will be decided based on the analysis done in the Jupyter notebook and R2 score, So please submit your notebooks Acknowledgements we are hosting this competition for the machine learning community to use for fun and practice.\n",
      "\n",
      "BART summarization:\n",
      "Folk Developers Community presents DataScience Hackathon Problem Statement. A US bike-sharing provider `XBikes` has a daily dataset on the rental bikes based on various environmental and seasonal settings. It wishes to use this data to understand the factors affecting the demand for these shared bikes.\n",
      "\n",
      "GPT2 summarization:\n",
      "Folk Developers Community presents DataScience Hackathon Problem Statement: A US bike-sharing provider `XBikes` has a daily dataset on the rental bikes based on various environmental and seasonal settings. Essentially, the company wants to know — Which variables are significant in predicting the demand for shared bikes.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(x[1]) for x in texts_5_10[:15]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10-20 предложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text id:\n",
      "13\n",
      "Source text:\n",
      "For this challenge, potential Facebook recruits will be exploring the map of the entire internet. Unlike the map of a city, where best routes are relatively fixed except for the occasional construction or parade detour, the paths that information travels  over the web are constantly changing. There is no centralized system of stop-lights or traffic cops.  Instead, there are tens of thousands of autonomous systems using a common protocol to advertise the next available hops, updated depending on service-agreements,  capacity, and load. This will be a test of both the candidates engineering know-how and their ability to statistically learn on complex, dynamic graph structures. The Task: you will be given a path which, at one point in the training time period, was an optimal path from node A to B. The question is then to make a probalistic prediction, for each of the 5 test graphs, whether the given path is STILL an optimal  path.  This is a much more difficult task than link prediction alone. The global structure of the graph may affect many optimal routes, paths can have varying lengths (and thus varying a priori probabilities of being optimal), and there may be multiple  optimal routes for a given source & destination. The Prize: Facebook is seeking data-savvy software engineers (Data Engineers) to build the next generation of systems that will transform the online experience of over a billion users. Appropriate candidates should have experience with multiple components  across the big data stack (check out the Visualization track for another way to highlight your skills).  There are many teams that they could be a fit for depending on their backgrounds . Positions are available in Menlo Park, Seattle, New York City, and London; candidates must have, or be eligible to obtain, authorization to work in the US or UK. An example visualization of Internet topology (round-trip times) produced by Walrus (courtesy of Young Hyun and inverted for display purposes)\n",
      "\n",
      "BERT summarization:\n",
      "For this challenge, potential Facebook recruits will be exploring the map of the entire internet. There is no centralized system of stop-lights or traffic cops. The Prize: Facebook is seeking data-savvy software engineers (Data Engineers) to build the next generation of systems that will transform the online experience of over a billion users.\n",
      "\n",
      "BART summarization:\n",
      "Facebook is seeking data-savvy software engineers (Data Engineers) to build the next generation of systems that will transform the online experience of over a billion users. Positions are available in Menlo Park, Seattle, New York City, and London; candidates must have, or be eligible to obtain, authorization to work in the US or UK.\n",
      "\n",
      "GPT2 summarization:\n",
      "For this challenge, potential Facebook recruits will be exploring the map of the entire internet. Instead, there are tens of thousands of autonomous systems using a common protocol to advertise the next available hops, updated depending on service-agreements,  capacity, and load. This is a much more difficult task than link prediction alone.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "17\n",
      "Source text:\n",
      "Kiva.org is an online microfinance site which allows people to lend money to borrowers in developing countries. Since 2008, lenders on Kiva.org can create and join (multiple) teams, and credit their lending activities to teams. According to a recent study,  joining teams on Kiva significantly (and positively) influences the lending behaviors of Kiva users. In this task, you will build a predictor for the team joining behavior on Kiva. Specifically, given a set of users, you are required to provide up to 10 teams for each user which he is likely to join in the near future, ranked by the probability of joining.  You are allowed to use any information in the historical (training) period, including the basic demographics of the user, the motivation statement, the past lending activities, and the teams the user joined in the past. Users, teams, and borrowers are de-identified  and indexed with integer IDs. You can make 10 submissions per day. Once you submit your results, you will get an accuracy score. This score will position you somewhere on the leaderboard. The evaluation metric is the mean average precision of your ranked lists of teams. A detailed description  of mean average precision can be found here: http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html. You can use any data mining techniques, any existing implementations, any combination of features, and either supervised or semi-supervised methods. Be creative in both the methods you select!\n",
      "\n",
      "BERT summarization:\n",
      "Kiva.org is an online microfinance site which allows people to lend money to borrowers in developing countries. According to a recent study,  joining teams on Kiva significantly (and positively) influences the lending behaviors of Kiva users. Users, teams, and borrowers are de-identified  and indexed with integer IDs.\n",
      "\n",
      "BART summarization:\n",
      "Kiva.org is an online microfinance site which allows people to lend money to borrowers in developing countries. According to a recent study,  joining teams on Kiva significantly (and positively) influences the lending behaviors of Kiva users. In this task, you will build a predictor for the team joining behavior of users. You can use any data mining techniques, any existing implementations, any combination of features, and either supervised or semi-supervised methods.\n",
      "\n",
      "GPT2 summarization:\n",
      "Kiva.org is an online microfinance site which allows people to lend money to borrowers in developing countries. In this task, you will build a predictor for the team joining behavior on Kiva.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "30\n",
      "Source text:\n",
      "About Us Colorado School Grades was created by a coalition of non-profit, community organizations that believe all children deserve access to a high-performing school. Our mission is to provide community members, parents, students, and educators with school performance  information that is both accessible and easy-to-understand. Our hope is that this will help families and students make more informed decisions about the school they choose based in some part on academic performance information. We also aim to inspire and equip  community members, parents, students, and educators with the information and resources they need to effectively engage in local school improvement efforts.  We provide resources for community stakeholders to improve their chosen schools. Colorado School Grades receives over 300,000 parents annually. Please use the site as a reference for any additional questions that you may have. And please be sure to include the Colorado School Grades link and logo in your visualization. Competition is organized and administered by Ryan Wilson at FiveFifty . Data Visualization Interests We believe that information is power and our work translates the state of Colorado’s school performance labels into easier-to-understand letter grades. We make these grades public on an intuitive, user-friendly platform at www.ColoradoSchoolGrades.com . Now that we have three years of letter grades for the schools, we are interested in seeing what trends and insights visualisation experts can identify and explain in compelling  data visualizations. Here is a list of some the questions we find intriguing: How have grades changed over time across the state (or perhaps more importantly how have they remained the same)? Where are the A schools primarily located? Our vision is that all kids have access to a high-performing school – how does the Colorado deliver against that promise of equity? Are there correlations between A schools and student demographics (free/reduced lunch is a proxy for poverty or by race) – Do poor and minority kids have access to A schools? Academic growth is an indicator used in the grading system. It is described in more detail in the data description page and on the Colorado School Grades website, but is perhaps the greatest indicator of how much teaching and learning is actually occurring  in the school. That said, where are the schools that have the best sub-grades for student growth? Are there particular schools that have high percentages of low income students AND high grades for student growth. Some may say those schools are doing more to  close Colorado’s achievement gap between the wealthy and the poor than any other. What percentage of Colorado’s student’s are ready for college and career by school or by school district? Which districts have the most A schools, F schools, or improving schools? Which schools have improved their letter grades the most? How do these grades, graduation rates, and college/career readiness metrics compare to labor market and economic data / needs? What have we missed? Please use your creativity to identify interesting trends or insights that the data tells us.\n",
      "\n",
      "BERT summarization:\n",
      "About Us Colorado School Grades was created by a coalition of non-profit, community organizations that believe all children deserve access to a high-performing school. Our mission is to provide community members, parents, students, and educators with school performance  information that is both accessible and easy-to-understand. Are there correlations between A schools and student demographics (free/reduced lunch is a proxy for poverty or by race) – Do poor and minority kids have access to A schools? Academic growth is an indicator used in the grading system. Are there particular schools that have high percentages of low income students AND high grades for student growth.\n",
      "\n",
      "BART summarization:\n",
      "Colorado School Grades is a coalition of non-profit, community organizations that believe all children deserve access to a high-performing school. Our mission is to provide community members, parents, students, and educators with school performance information that is both accessible and easy-to-understand. We are interested in seeing what trends and insights visualisation experts can identify and explain in compelling data visualizations.\n",
      "\n",
      "GPT2 summarization:\n",
      "About Us Colorado School Grades was created by a coalition of non-profit, community organizations that believe all children deserve access to a high-performing school. Our mission is to provide community members, parents, students, and educators with school performance  information that is both accessible and easy-to-understand. We make these grades public on an intuitive, user-friendly platform at www.ColoradoSchoolGrades.com . Are there correlations between A schools and student demographics (free/reduced lunch is a proxy for poverty or by race) – Do poor and minority kids have access to A schools? It is described in more detail in the data description page and on the Colorado School Grades website, but is perhaps the greatest indicator of how much teaching and learning is actually occurring  in the school.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "47\n",
      "Source text:\n",
      "Two of Kaggle's very own are presenting an introductory tutorial at Strata 2013. Targeted for those with basic programming experience, it will cover the end-to-end analysis of predictive data problems. The tutorial is comprised of four sections, the last of which is this, a hands-on Kaggle competition in which participants can experience firsthand the joys of creating a model and the sorrows of overfitting. Competition ends: 9:00am Tuesday, 02/26/2013 Location: Ballroom AB Competition Starts: Approximately 11:15 AM PT (2:15PM ET), 02/26/2013 Competition Ends: 12:30 PM PT (3:30 PM ET) , 02/26/2013 Open to the public! Yes, you can participate in this for-fun competition without attending the tutorial. For fun? You heard correctly; there's no Kaggle points or money up for grabs here. Isn't getting to lunch early a big enough motivation? With just over an hour from start to finish, this competition is going to be a sprint. Unlimited* submissions! Show our servers who's boss! *for small values of unlimited Where's the data? To prevent head starts, the data will be available at the start of the competition. About the presenters Ben Hamner has worked with machine learning problems in a variety of different domains, including natural language processing, computer vision, web classification, and neuroscience. Prior to joining Kaggle, he applied machine learning to improve brain-computer interfaces as a Whitaker Fellow at the École Polytechnique Fédérale de Lausanne in Lausanne, Switzerland. He graduated with BSE in Biomedical Engineering, Electrical Engineering, and Math from Duke University. William Cukierski has a bachelor’s degree in physics from Cornell University and a Ph.D. in biomedical engineering from Rutgers University, where he studied applications of machine learning in cancer research.\n",
      "\n",
      "BERT summarization:\n",
      "Two of Kaggle's very own are presenting an introductory tutorial at Strata 2013. Targeted for those with basic programming experience, it will cover the end-to-end analysis of predictive data problems. You heard correctly; there's no Kaggle points or money up for grabs here.\n",
      "\n",
      "BART summarization:\n",
      "Two of Kaggle's very own are presenting an introductory tutorial at Strata 2013. Targeted for those with basic programming experience, it will cover the end-to-end analysis of predictive data problems. The tutorial is comprised of four sections, the last of which is this, a hands-on Kaggling competition.\n",
      "\n",
      "GPT2 summarization:\n",
      "Two of Kaggle's very own are presenting an introductory tutorial at Strata 2013. To prevent head starts, the data will be available at the start of the competition. William Cukierski has a bachelor’s degree in physics from Cornell University and a Ph.D. in biomedical engineering from Rutgers University, where he studied applications of machine learning in cancer research.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "48\n",
      "Source text:\n",
      "When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. This access may allow an employee to read/manipulate resources through various applications or web portals. It is assumed that employees fulfilling the functions of a given role will access the same or similar resources. It is often the case that employees figure out the access they need as they encounter roadblocks during their daily work (e.g. not able to log into a reporting portal). A knowledgeable supervisor then takes time to manually grant the needed access in order to overcome access obstacles. As employees move throughout a company, this access discovery/recovery cycle wastes a nontrivial amount of time and money. There is a considerable amount of data regarding an employee’s role within an organization and the resources to which they have access. Given the data related to current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company. These auto-access models seek to minimize the human involvement required to grant or revoke employee access. Objective The objective of this competition is to build a model, learned using historical data, that will determine an employee's access needs, such that manual access transactions (grants and revokes) are minimized as the employee's attributes change over time. The model will take an employee's role information and a resource code and will return whether or not access should be granted. Partners This competition is hosted in collaboration with the IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2013)\n",
      "\n",
      "BERT summarization:\n",
      "When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. There is a considerable amount of data regarding an employee’s role within an organization and the resources to which they have access. Objective The objective of this competition is to build a model, learned using historical data, that will determine an employee's access needs, such that manual access transactions (grants and revokes) are minimized as the employee's attributes change over time.\n",
      "\n",
      "BART summarization:\n",
      "When an employee starts work, they first need to obtain the computer access necessary to fulfill their role. It is often the case that employees figure out the access they need as they encounter roadblocks during their daily work. A knowledgeable supervisor then takes time to manually grant the needed access. This access discovery/recovery cycle wastes a nontrivial amount of time and money.\n",
      "\n",
      "GPT2 summarization:\n",
      "When an employee at any company starts work, they first need to obtain the computer access necessary to fulfill their role. Given the data related to current employees and their provisioned access, models can be built that automatically determine access privileges as employees enter and leave roles within a company. Partners This competition is hosted in collaboration with the IEEE International Workshop on Machine Learning for Signal Processing (MLSP 2013)\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "50\n",
      "Source text:\n",
      "This competition was launched under the Kaggle Startup Program .  If you're a startup with a predictive modelling challenge, please apply ! Adzuna wants to build a prediction engine for the salary of any UK job ad, so they can make huge improvements in the experience of users searching for jobs, and help employers and jobseekers  figure out the market worth of different positions. At the moment, approximately half of the UK job ads they index have a salary publicly displayed.  They need your help to bring more transparency to this important market. Adzuna has a large dataset (hundreds of thousands of records), which is mostly unstructured text, with a few structured data fields. These can be in a number of different formats because of the hundreds of different sources  of records. Adzuna needs the help of the Kaggle community to figure out the best techniques to apply to this data set to build a highly accurate predictive model for new ads. You will build, train and test your salary  prediction engines against a wide field of competitors. As an added perk, Adzuna intends to implement their chosen model on their website, both in the UK and worldwide. You will have the satisfaction of seeing your work implemented in production and change the way people search for jobs in the future. Successful models will incorporate some analysis of the impact of including different keywords or phrases, as well as making use of the structured data fields like location, hours or company.  Some of the structured data shown (such as category) is 'inferred'  by Adzuna's own processes, based on where an ad came from or its contents, and may not be \"correct\" but is representative of the real data. You will be provided with a training data set on which to build your model, which will include all variables including salary.  A second data set will be used to provide feedback on the public leaderboard.  After approximately 6 weeks, Kaggle will release  a final data set that does not include the salary field to participants, who will then be required to submit their salary predictions against each job for evaluation.\n",
      "\n",
      "BERT summarization:\n",
      "This competition was launched under the Kaggle Startup Program . Successful models will incorporate some analysis of the impact of including different keywords or phrases, as well as making use of the structured data fields like location, hours or company. You will be provided with a training data set on which to build your model, which will include all variables including salary. After approximately 6 weeks, Kaggle will release  a final data set that does not include the salary field to participants, who will then be required to submit their salary predictions against each job for evaluation.\n",
      "\n",
      "BART summarization:\n",
      "Adzuna wants to build a prediction engine for the salary of any UK job ad. At the moment, approximately half of the UK job ads they index have a salary publicly displayed. You will be provided with a training data set on which to build your model, which will include all variables including salary.\n",
      "\n",
      "GPT2 summarization:\n",
      "This competition was launched under the Kaggle Startup Program . You will have the satisfaction of seeing your work implemented in production and change the way people search for jobs in the future. After approximately 6 weeks, Kaggle will release  a final data set that does not include the salary field to participants, who will then be required to submit their salary predictions against each job for evaluation.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "53\n",
      "Source text:\n",
      "Read the summary of the competition for a quick overview of the impact of the results. We depend on shipping industry's uninterrupted ability to transport goods across long distances. Navigation technologies combine accurate position and environmental data to calculate optimal transport routes. Accounting for and reducing the impact of commercial shipping on the ocean’s environment, while achieving commercial sustainability, is of increasing importance, especially as it relates to the influence of cumulative noise “footprints” on the great whales. Marinexplore is organizing the Planet's ocean data with the leading community of ocean professionals. One of the important datasets consists of acoustic recordings that can be used to detect species inhabiting the global ocean. Knowledge about animal locations can be utilized in industrial operations. Cornell University 's Bioacoustic Research Program has extensive experience in identifying endangered whale species and has deployed a 24/7 buoy network to guide ships from colliding with the world's last 400 North Atlantic right whales. Illustration of ships navigating safely around the habitat of whales. Right whales make a half-dozen types of sounds, but the characteristic up-call is the one identified by the auto-detection buoys. The up-call is useful because it’s distinctive and right whales give it often. A type of “contact call,” the up-call is a little like small talk--the sound of a right whale going about its day and letting others know it’s nearby. In this recording the up-call is easy to hear--a deep, rising “whoop” that lasts about a second: Right whale up-call Marinexplore and Cornell researchers challenge YOU to beat the existing whale detection algorithm identifying the right whale calls. This will advance ship routing decisions in the region. [For details on the buoy network see a paper published by Acoustical Society of America.] Read the summary of the competition for a quick overview of the impact of the results.\n",
      "\n",
      "BERT summarization:\n",
      "Read the summary of the competition for a quick overview of the impact of the results. Navigation technologies combine accurate position and environmental data to calculate optimal transport routes. Right whales make a half-dozen types of sounds, but the characteristic up-call is the one identified by the auto-detection buoys.\n",
      "\n",
      "BART summarization:\n",
      "Right whales make a half-dozen types of sounds, but the characteristic up-call is the one identified by the auto-detection buoys. Marinexplore and Cornell researchers challenge YOU to beat the existing whale detection algorithm identifying the right whale calls. This will advance ship routing decisions in the region.\n",
      "\n",
      "GPT2 summarization:\n",
      "Read the summary of the competition for a quick overview of the impact of the results. We depend on shipping industry's uninterrupted ability to transport goods across long distances. One of the important datasets consists of acoustic recordings that can be used to detect species inhabiting the global ocean. In this recording the up-call is easy to hear--a deep, rising “whoop” that lasts about a second: Right whale up-call Marinexplore and Cornell researchers challenge YOU to beat the existing whale detection algorithm identifying the right whale calls.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "62\n",
      "Source text:\n",
      "With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. Currently, only a handful of very experienced researchers can identify individual whales on sight while out on the water. For the majority of researchers, identifying individual whales takes time, making it difficult to effectively target whales for biological samples, acoustic recordings, and necessary health assessments. To track and monitor the population, right whales are photographed during aerial surveys and then manually matched to an online photo-identification catalog. Customized software has been developed to aid in this process ( DIGITS ), but this still relies on a manual inspection of the potential comparisons, and there is a lag time for those images to be incorporated into the database. The current identification process is extremely time consuming and requires special training. This constrains marine biologists, who work under tight deadlines with limited budgets. This competition challenges you to automate the right whale recognition process using a dataset of aerial photographs of individual whales. Automating the identification of right whales would allow researchers to better focus on their conservation efforts. Recognizing a whale in real-time would also give researchers on the water access to potentially life-saving historical health and entanglement records as they struggle to free a whale that has been accidentally caught up in fishing gear. Acknowledgements MathWorks is sponsoring the competition prize pool. If your team is participating in this competition MathWorks is also providing complimentary software. Click here for more details on how to request your copy. Thanks to Christin Khan and Leah Crowe from NOAA for hand labeling the images to create this one of a kind dataset and to the right whale research team at New England Aquarium for maintaining the photo-identification catalog . Without their continued efforts, none of this would be possible.\n",
      "\n",
      "BERT summarization:\n",
      "With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. The current identification process is extremely time consuming and requires special training. Click here for more details on how to request your copy.\n",
      "\n",
      "BART summarization:\n",
      "There are fewer than 500 North Atlantic right whales left in the world's oceans. To track and monitor the population, right whales are photographed during aerial surveys and then manually matched to an online photo-identification catalog. This competition challenges you to automate the right whale recognition process using a dataset of aerial photographs of individual whales.\n",
      "\n",
      "GPT2 summarization:\n",
      "With fewer than 500 North Atlantic right whales left in the world's oceans, knowing the health and status of each whale is integral to the efforts of researchers working to protect the species from extinction. For the majority of researchers, identifying individual whales takes time, making it difficult to effectively target whales for biological samples, acoustic recordings, and necessary health assessments. If your team is participating in this competition MathWorks is also providing complimentary software.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "72\n",
      "Source text:\n",
      "For many users, the idea of sorting through hundreds of thousands of online dating profiles to find potential matches seems daunting. Instead, it would be great to have an automated system that recommends profiles of other users that a user will like. One way to accomplish this is to build a recommendation system that predicts the profiles a user is likely to enjoy based upon the user's past ratings of other profiles. To build such a recommender system, we will be working with a small subset of profile rating data from the Czech dating site http://libimseti.cz/ . The training data you are given consists of 3,279,759 ratings of 10,000 profiles by 10,000 users, yielding about 97% missing ratings. The ratings are integers between 1 and 10 with 10 being the best. Each user in the data set has rated at least 25 profiles.  Your objective is to use the available ratings for each user to predict the missing ratings, thus recommending dating profiles that the user will enjoy. This type of problem goes by many names in different fields: ``recommender system'', ``collaborative filtering'', ``matrix completion'', or ``missing data imputation''. Beyond the ratings, you are also given the gender of each user. No additional information may be used to build your predictive models. Acknowledgements Data for this competition has been made available from Vaclav Petricek: Lukas Brozovsky and Vaclav Petricek, \"Recommender System for Online Dating Service\", In Proceedings of Conference Znalosti, 2007. Thank you!\n",
      "\n",
      "BERT summarization:\n",
      "For many users, the idea of sorting through hundreds of thousands of online dating profiles to find potential matches seems daunting. To build such a recommender system, we will be working with a small subset of profile rating data from the Czech dating site http://libimseti.cz/ . The ratings are integers between 1 and 10 with 10 being the best.\n",
      "\n",
      "BART summarization:\n",
      "The competition is looking for a recommender system that predicts the profiles a user is likely to enjoy based upon the user's past ratings of other profiles. The training data you are given consists of 3,279,759 ratings of 10,000 profiles by10,000 users, yielding about 97% missing ratings. Each user in the data set has rated at least 25 profiles.\n",
      "\n",
      "GPT2 summarization:\n",
      "For many users, the idea of sorting through hundreds of thousands of online dating profiles to find potential matches seems daunting. To build such a recommender system, we will be working with a small subset of profile rating data from the Czech dating site http://libimseti.cz/ . Your objective is to use the available ratings for each user to predict the missing ratings, thus recommending dating profiles that the user will enjoy.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "74\n",
      "Source text:\n",
      "You have seen in class that simple linear models can be very powerful in predicting complex functions. In this task, you are asked to build a model that predicts the delay in microseconds that a processor requires to execute a fixed portion of a program given a set of microarchitectural configurations. The performance of a processor can greatly vary when configurations such as cache size and register file size are varied. The extent of this variation depends on how the program exploits these characteristics. Fourteen different microarchitectural parameters can be varied across a range of values, and the delay of the processor can be measured under such conditions. The relationship between microarchitectural characteristics and processor delay might be non-linear, and the given observations might be noisy. However, you should build a linear regressor using the techniques learned in class to find a compromise between complexity and accuracy in predicting the given training set, to avoid overfitting in the presence of noise and in the lack of abundant training data. To be able to model the non-linear relationship in the given data set, try computing new features from the given ones and adding them to the feature space. The hope is that in this extended feature space, a linear relationship between the inputs and the output variable can be found. Note that the more complex your feature space becomes, the more prone you are to overfitting to noise since you are only given a finite data set to train your regressor. Forming teams and score mapping In order to map your final score from the Kaggle competition back to the ETHZ grading system of the course, you must use your @ethz.ch email address for the Kaggle user account used during the competition. It is allowed to solve this project in teams with up to three team members. Please form the teams from inside of the Kaggle project page yourself. For this, someone from your team: Opens the Kaggle project page Select \"My Team\" from navigation on the left hand side Enters the \"Team Name\" in the field Under \"Invite someone to join your team\" puts in the emails from the other team mates and then clicks \"Send\" IF you are looking for team mates and cannot find someone during the lecture, feel free to use the Kaggle forum of the project to look for other team members. You find the forum by clicking on \"Forum\" from the Kaggle project page. Note: While it is possible to also form teams later during the project, we recommend forming the teams right away to avoid problems with the total amount of made submission when merging a team. For more information see: https://www.kaggle.com/wiki/FormingATeam\n",
      "\n",
      "BERT summarization:\n",
      "You have seen in class that simple linear models can be very powerful in predicting complex functions. You find the forum by clicking on \"Forum\" from the Kaggle project page. Note: While it is possible to also form teams later during the project, we recommend forming the teams right away to avoid problems with the total amount of made submission when merging a team.\n",
      "\n",
      "BART summarization:\n",
      "You are asked to build a model that predicts the delay in microseconds that a processor requires to execute a fixed portion of a program. The relationship between microarchitectural characteristics and processor delay might be non-linear, and the given observations might be noisy. You should build a linear regressor using the techniques learned in class to find a compromise between complexity and accuracy.\n",
      "\n",
      "GPT2 summarization:\n",
      "You have seen in class that simple linear models can be very powerful in predicting complex functions. Fourteen different microarchitectural parameters can be varied across a range of values, and the delay of the processor can be measured under such conditions. The hope is that in this extended feature space, a linear relationship between the inputs and the output variable can be found. Forming teams and score mapping In order to map your final score from the Kaggle competition back to the ETHZ grading system of the course, you must use your @ethz.ch email address for the Kaggle user account used during the competition.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(x[1]) for x in texts_10_20[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20-30 предложений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text id:\n",
      "5\n",
      "Source text:\n",
      "Your success depends upon how closely you can deliver scores to those of human expert graders. For this competition, there are 5 essay sets. Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information  and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the data sets has its own unique characteristics. The variability is intended to test the  limits of your scoring engine's capabilities. The data has these columns: id : A unique identifier for each individual student essay set : 1-5, an id for each set of essays essay : The ascii text of a student's response rater1 : Rater 1's grade rater2 : Rater 2's grade grade : Resolved score between the raters In addition, a Microsoft Word 2010 Readme file describes each essay set. The Readme file contains the prompt from which the essays were generated. If applicable, the Readme file also includes the source information for essays that required students to  read and respond to an excerpt. Anonymization We have made an effort to remove personally identifying information from the essays using the Named Entity Recognizer (NER) from the Stanford Natural Language Processing group and  a variety of other approaches. The relevant entities are identified in the text and then replaced with a string such as \"@PERSON1.\" The entitities identified by NER are: \"PERSON\", \"ORGANIZATION\", \"LOCATION\", \"DATE\", \"TIME\", \"MONEY\", \"PERCENT\" Other replacements made: \"MONTH\" (any month name not tagged as a date by the NER), \"EMAIL\" (anything that looks like an e-mail address), \"NUM\" (word containing digits or non-alphanumeric symbols), and \"CAPS\" (any capitalized word that doesn't  begin a sentence, except in essays where more than 20% of the characters are capitalized letters), \"DR\" (any word following \"Dr.\" with or without the period, with any capitalization, that doesn't fall into any of the above), \"CITY\" and \"STATE\" (various cities  and states). Here are some hypothetical examples of replacements made: \"I attend Springfield School...\" --> \"...I attend @ORGANIZATION1\" \"once my family took my on a trip to Springfield.\" --> \"once my family took me on a trip to @LOCATION1\" \"John Doe is a person, and so is Jane Doe. But if I talk about Mr. Doe, I can't tell that's the same person.\" --> \"...@PERSON1 is a person, and so is @PERSON2. But if you talk about @PERSON3, I can't tell that's the same person.\" \"...my phone number is 555-2106\" --> \"...my phone number is @NUM1\" Any words appearing in the prompt or source material for the corresponding essay set were white-listed and not anonymized.\n",
      "\n",
      "BERT summarization:\n",
      "Your success depends upon how closely you can deliver scores to those of human expert graders. Each of the sets of essays was generated from a single prompt. Each of the data sets has its own unique characteristics. The Readme file contains the prompt from which the essays were generated. my phone number is 555-2106\" --> \"...my phone number is @NUM1\" Any words appearing in the prompt or source material for the corresponding essay set were white-listed and not anonymized.\n",
      "\n",
      "BART summarization:\n",
      "There are 5 essay sets. Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored.\n",
      "\n",
      "GPT2 summarization:\n",
      "Your success depends upon how closely you can deliver scores to those of human expert graders. The variability is intended to test the  limits of your scoring engine's capabilities. If applicable, the Readme file also includes the source information for essays that required students to  read and respond to an excerpt. But if you talk about @PERSON3, I can't tell that's the same person.\" \"...\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "12\n",
      "Source text:\n",
      "Congratulations! You are presented the opportunity of a lifetime: shaping the social protection agenda of Tanzania! The Board of Directors of the National Bureau of Statistics of Tanzania has tasked you with elaborating a targeting system for the social programs the government plans to implement over the next couple of years. More specifically, you are being asked to focus on determining what information should be collected to assess whether a household is likely to be in need of social assistance in the future. Your assessment should provide a rigorous quantitative analysis using the National Panel Surveys of 2008–2009 and 2010–2011, nationally representative household panel surveys in Tanzania. The final product should consist of: A brief characterization of the poor in Tanzania, including: Summary of key poverty indicators. The goal is to provide a broad overview of poverty. Detailed description of the poor (e.g. gender, age, household size, region, education, ethnicity, employment). This will provide an idea of who the poor are. Differences in characteristics between poor and non-poor (e.g. education level, ownership of assets, etc). The goal is to assess potential explanations and/or consequences of poverty. An analysis of which household and individual characteristics should be collected in the beneficiary questionnaire. This analysis should be based on an algorithm you develop to determine which set of variables have the most predictive power in terms of whether the household will likely be in need of assistance. Additional instructions for developing and testing this algorithm are presented in the appendix (at the end of this document). A Social Assistance Targeting questionnaire that the government of Tanzania can actually use in the field to assess who will be in most need of assistance. The items on the form should be based on the algorithm you develop in #2, but also consider the practical implications of collecting the information implied by the algorithm. The form itself can be presented in the appendix but the justification for which items appear on the form should be in the text of the memo. A brief assessment of whether and how to implement such targeting system in the Tanzanian context. This should take into consideration the tradeoffs between the cost of targeting and benefits from focusing assistance on a population subgroup given the poverty profile you sketched in (1). The final product should be a ten-page document, comprising a one-page executive summary, a four-page memo and a five-page technical appendix. While people in the high levels of the National Bureau of Statistics have technical backgrounds, write the memo in a way that is suitable to a broad array of policymakers that will be engaged in the discussion of this issue, and leave the technical analysis to the appendix. The appendix should contain the details of the statistical analyses you conducted for (1) and (2), including a description of the exact algorithm/formula you propose using to determine if a person is in need of social assistance, and the process you employed to arrive at such algorithm. It should also contain the form you developed in (3). Remember that this algorithm will be used to determine who gets social assistance in Tanzania and who does not, so the stakes are very high. The National Bureau of Statistics will provide you with extensive support during the course of this assignment. You have been assigned to 3 mentors, Paola Vargas, Mahnaz Islam and Dan Levy, all of whom will be available during their usual consultation hours. The National Bureau of Statistics also coordinates an on-line forum, through which you can share ideas with your colleagues and your mentors. We wish you the best of luck in this crucial assignment for the government of Tanzania!\n",
      "\n",
      "BERT summarization:\n",
      "You are presented the opportunity of a lifetime: shaping the social protection agenda of Tanzania! The Board of Directors of the National Bureau of Statistics of Tanzania has tasked you with elaborating a targeting system for the social programs the government plans to implement over the next couple of years. The goal is to provide a broad overview of poverty. This will provide an idea of who the poor are. An analysis of which household and individual characteristics should be collected in the beneficiary questionnaire. This should take into consideration the tradeoffs between the cost of targeting and benefits from focusing assistance on a population subgroup given the poverty profile you sketched in (1).\n",
      "\n",
      "BART summarization:\n",
      "The Board of Directors of the National Bureau of Statistics of Tanzania has tasked you with elaborating a targeting system for the social programs the government plans to implement over the next couple of years. You are being asked to focus on determining what information should be collected to assess whether a household is likely to be in need of social assistance in the future.\n",
      "\n",
      "GPT2 summarization:\n",
      "You are presented the opportunity of a lifetime: shaping the social protection agenda of Tanzania! Detailed description of the poor (e.g. gender, age, household size, region, education, ethnicity, employment). This analysis should be based on an algorithm you develop to determine which set of variables have the most predictive power in terms of whether the household will likely be in need of assistance. A brief assessment of whether and how to implement such targeting system in the Tanzanian context. The appendix should contain the details of the statistical analyses you conducted for (1) and (2), including a description of the exact algorithm/formula you propose using to determine if a person is in need of social assistance, and the process you employed to arrive at such algorithm.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "15\n",
      "Source text:\n",
      "Understanding how and why we are here is one of the fundamental questions for the human race. Part of the answer to this question lies in the origins of galaxies, such as our own Milky Way. Yet questions remain about how the Milky Way (or any of the other ~100 billion galaxies in our Universe) was formed and has evolved. Galaxies come in all shapes, sizes and colors: from beautiful spirals to huge ellipticals. Understanding the distribution, location and types of galaxies as a function of shape, size, and color are critical pieces for solving this puzzle. The Whirlpool Galaxy (M51). Credit: NASA and European Space Agency With each passing day telescopes around and above the Earth capture more and more images of distant galaxies. As better and bigger telescopes continue to collect these images, the datasets begin to explode in size. In order to better understand how the different shapes (or morphologies ) of galaxies relate to the physics that create them, such images need to be sorted and classified. Kaggle has teamed up with Galaxy Zoo and Winton Capital to produce the Galaxy Challenge, where participants will help classify galaxies into categories. Image Credit: ESA/Hubble & NASA Galaxies in this set have already been classified once through the help of hundreds of thousands of volunteers, who collectively classified the shapes of these images by eye in a successful citizen science crowdsourcing project. However, this approach becomes less feasible as data sets grow to contain of hundreds of millions (or even billions) of galaxies. That's where you come in. This competition asks you to analyze the JPG images of galaxies to find automated metrics that reproduce the probability distributions derived from human classifications. For each galaxy, determine the probability that it belongs in a particular class. Can you write an algorithm that behaves as well as the crowd does? Contributors: D. Harvey, C. Lintott, T. Kitching, P. Marshall, K. Willett, Galaxy Zoo Acknowledgments The Contributors and the rest of the Galaxy Zoo and Kaggle teams would like to say a big thank you to Winton Capital for helping make this happen. Without their support, we would have not been able to make this competition go ahead.\n",
      "\n",
      "BERT summarization:\n",
      "Understanding how and why we are here is one of the fundamental questions for the human race. Image Credit: ESA/Hubble & NASA Galaxies in this set have already been classified once through the help of hundreds of thousands of volunteers, who collectively classified the shapes of these images by eye in a successful citizen science crowdsourcing project. However, this approach becomes less feasible as data sets grow to contain of hundreds of millions (or even billions) of galaxies.\n",
      "\n",
      "BART summarization:\n",
      "Galaxies come in all shapes, sizes and colors: from beautiful spirals to huge ellipticals. Kaggle has teamed up with Galaxy Zoo and Winton Capital to produce the Galaxy Challenge. Participants will help classify galaxies into categories. Can you write an algorithm that behaves as well as the crowd does?\n",
      "\n",
      "GPT2 summarization:\n",
      "Understanding how and why we are here is one of the fundamental questions for the human race. In order to better understand how the different shapes (or morphologies ) of galaxies relate to the physics that create them, such images need to be sorted and classified. Contributors: D. Harvey, C. Lintott, T. Kitching, P. Marshall, K. Willett, Galaxy Zoo Acknowledgments The Contributors and the rest of the Galaxy Zoo and Kaggle teams would like to say a big thank you to Winton Capital for helping make this happen.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "41\n",
      "Source text:\n",
      "This competition will launch at midnight UTC on Saturday, December 15. Santa Claus was excited to learn about the Kaggle competition platform, and wanted to use it for a slightly different purpose. Rather than a predictive modeling problem, he has an optimization problem for you: a very, very important optimization problem. Santa needs help choosing the route he takes when delivering presents around the globe. Every year, Santa has to visit every boy and girl on his list.  It's a tough challenge, and Santa admits he scored a B- on his combinatorical optimization final. He's  hoping you can develop algorithms that will solve his problem year after year. Santa asked that we give you one particular instance of his TSP (Traveling Santa Problem). However, Santa's dilemma isn't quite the same as the Traveling Salesman Problem with which you may be familiar. Santa likes to see new terrain every year--don't ask,  it's a reindeer thing--and doesn't want his route to be predictable. You're looking for shortest-distance paths through a set of chimneys, but instead of providing one path, Santa asks you to provide two disjoint paths. If one of your paths contains an edge from A to B, your other path must not contain  an edge from A to B or from B to A (either order still counts as using that edge). Your score is the larger of the two distances.  Santa asks competition winners to publish and open source the algorithms they use (for his future use, of course). Rudolph was very adament about minimizing his workload. Trust us, you don't want to be on Rudolph's bad side. Important note about prizes: We believe that Kaggle's public leaderboard is very important for both the fun of the competition and achieving great results, and we want to provide an incentive for everyone to submit to the public leaderboard  all along the way (even though you can easily determine your submission's score all by yourself). So the competition will have two sets of prizes , one based on the scores at the end of the competition, and one based on the scores at  the end of a randomly chosen day (UTC) between December 23 and January 17. The day will not be revealed (or even chosen) until after the competition ends. (The competition will end at the end of the day UTC on January 18.) Attributions: Data generation and lots of help framing the problem (including coming up with this TSP variant): Robert Bosch of Oberlin College Math Department Santa photo: AurélienS Sleigh photo: Creative Tools Globe: William Cook\n",
      "\n",
      "BERT summarization:\n",
      "This competition will launch at midnight UTC on Saturday, December 15. Santa needs help choosing the route he takes when delivering presents around the globe. Important note about prizes: We believe that Kaggle's public leaderboard is very important for both the fun of the competition and achieving great results, and we want to provide an incentive for everyone to submit to the public leaderboard  all along the way (even though you can easily determine your submission's score all by yourself). The day will not be revealed (or even chosen) until after the competition ends. ( Attributions: Data generation and lots of help framing the problem (including coming up with this TSP variant): Robert Bosch of Oberlin College Math Department Santa photo: AurélienS Sleigh photo: Creative Tools Globe: William Cook\n",
      "\n",
      "BART summarization:\n",
      "Kaggle's Santa Claus competition will launch on Saturday, December 15. Santa needs help choosing the route he takes when delivering presents around the globe. You're looking for shortest-distance paths through a set of chimneys. If one of your paths contains an edge from A to B, your other path must not.\n",
      "\n",
      "GPT2 summarization:\n",
      "This competition will launch at midnight UTC on Saturday, December 15. Santa Claus was excited to learn about the Kaggle competition platform, and wanted to use it for a slightly different purpose. You're looking for shortest-distance paths through a set of chimneys, but instead of providing one path, Santa asks you to provide two disjoint paths. The competition will end at the end of the day UTC on January 18.)\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "56\n",
      "Source text:\n",
      "In this practical you will compete against your classmates to build the best performing part of speech tagger. The main aim will be to design and implement a Hidden Markov Model tagger of the type described in lecture 2. Due to your lecturer’s over  indulgence of BBC crime drama, you will train and test your tagger for Danish text. No knowledge of Danish is required to do well in this practical, though you may take the opportunity to learn about Danish morphology in order to develop innovative strategies  for tagging unknown words (the Wikipedia page on Danish Grammar is a good start). Task 1: Submit a basic first order HMM tagger In order to implement your tagger you will need to write a program which estimates the quantities P ( tag i | tag i − 1 ) and P ( word | tag ) by counting instances in the training data, and working out relative frequencies as described in the lectures. You may at this stage want to think what to do about unobserved tag transitions: there is an extensive literature on ‘smoothing’  for HMMs, but a simple method is to add 1 to each transition counts and then treat unobserved transitions as if they had actually been seen once (Google for “add 1 smoothing” or “Laplace smoothing”). Next you will have to implement the Viterbi algorithm for tagging new sentences. You may want to extend it a little to cope with the case where you encounter an unknown word. Your technique for handling unknown words will be crucial to your taggers performance. When you have built the training and tagging routines for your model, test its accuracy by a technique called 10-fold ‘cross-validation’. To do this you split the training corpus 90%/10% into training and test sections. Train your tagger on the 90%  and test on the 10%. Now take a different 90%/10% split and do the same, and so on, rotating the 90%/10% split so that eventually each bit of the corpus has been used as testing data. You can then average the results for each 10% test set. You can use this  testing technique to debug your code and to test your extensions to the tagger. Once you are satisfied with your taggers performance you should use it to tag the test data supplied and submit the output to competition website. You may make up to five competition submissions per day. You can use whatever programming language you like, but make sure that your code is fully and clearly documented so that someone who may not be completely fluent with that language (i.e. the demonstrator) can understand what you have done. Task 2: Win the competitions by exploring more advanced tagging algorithms Once you have implemented a basic HMM you should then explore extensions to your system to improve its accuracy. Possible extension include implementing a second order HMM, more advanced smoothing, and exploring the feature based strategies for tagging  unknown words discussed in the lectures.\n",
      "\n",
      "BERT summarization:\n",
      "In this practical you will compete against your classmates to build the best performing part of speech tagger. Task 1: Submit a basic first order HMM tagger In order to implement your tagger you will need to write a program which estimates the quantities P ( tag i | tag i − 1 ) and P ( word | tag ) by counting instances in the training data, and working out relative frequencies as described in the lectures. You may want to extend it a little to cope with the case where you encounter an unknown word. To do this you split the training corpus 90%/10% into training and test sections. You may make up to five competition submissions per day.\n",
      "\n",
      "BART summarization:\n",
      "In this practical you will compete against your classmates to build the best performing part of speech tagger. The main aim will be to design and implement a Hidden Markov Model tagger of the type described in lecture 2. No knowledge of Danish is required to do well in this practical, though you may take the opportunity to learn about Danish morphology.\n",
      "\n",
      "GPT2 summarization:\n",
      "In this practical you will compete against your classmates to build the best performing part of speech tagger. Next you will have to implement the Viterbi algorithm for tagging new sentences. When you have built the training and tagging routines for your model, test its accuracy by a technique called 10-fold ‘cross-validation’.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "57\n",
      "Source text:\n",
      "In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat.  This is easy for humans, dogs, and cats. Your computer will find it a bit more difficult. Deep Blue beat Kasparov at chess in 1997. Watson beat the brightest trivia minds at Jeopardy in 2011. Can you tell Fido from Mittens in 2013? The Asirra data set Web services are often protected with a challenge that's supposed to be easy for people to solve, but difficult for computers. Such a challenge is often called a CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof). HIPs are used for many purposes, such as to reduce email and blog spam and prevent brute-force attacks on web site passwords. Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. Many even think it's fun! Here is an example of the Asirra interface: Asirra is unique because of its partnership with Petfinder.com , the world's largest site devoted to finding homes for homeless pets. They've provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States. Kaggle is fortunate to offer a subset of this data for fun and research. Image recognition attacks While random guessing is the easiest form of attack, various forms of image recognition can allow an attacker to make guesses that are better than random. There is enormous diversity in the photo database (a wide variety of backgrounds, angles, poses, lighting, etc.), making accurate automatic classification difficult. In an informal poll conducted many years ago, computer vision experts posited that a classifier with better than 60% accuracy would be difficult without a major advance in the state of the art. For reference, a 60% classifier improves the guessing probability of a 12-image HIP from 1/4096 to 1/459. State of the art The current literature suggests machine classifiers can score above 80% accuracy on this task [1] . Therfore, Asirra is no longer considered safe from attack.  We have created this contest to benchmark the latest computer vision and deep learning approaches to this problem. Can you crack the CAPTCHA? Can you improve the state of the art? Can you create lasting peace between cats and dogs? Okay, we'll settle for the former. Acknowledgements We extend our thanks to Microsoft Research for providing the data for this competition. Jeremy Elson, John R. Douceur, Jon Howell, Jared Saul, Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization, in Proceedings of 14th ACM Conference on Computer and Communications Security (CCS), Association for Computing Machinery, Inc., Oct. 2007\n",
      "\n",
      "BERT summarization:\n",
      "In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat. Your computer will find it a bit more difficult. Here is an example of the Asirra interface: Asirra is unique because of its partnership with Petfinder.com , the world's largest site devoted to finding homes for homeless pets. Kaggle is fortunate to offer a subset of this data for fun and research. Image recognition attacks While random guessing is the easiest form of attack, various forms of image recognition can allow an attacker to make guesses that are better than random.\n",
      "\n",
      "BART summarization:\n",
      "In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat. This is easy for humans, dogs, and cats. Your computer will find it a bit more difficult. The current literature suggests machine classifiers can score above 80% accuracy on this task.\n",
      "\n",
      "GPT2 summarization:\n",
      "In this competition, you'll write an algorithm to classify whether images contain either a dog or a cat. Here is an example of the Asirra interface: Asirra is unique because of its partnership with Petfinder.com , the world's largest site devoted to finding homes for homeless pets. State of the art The current literature suggests machine classifiers can score above 80% accuracy on this task [1] . Jeremy Elson, John R. Douceur, Jon Howell, Jared Saul, Asirra: A CAPTCHA that Exploits Interest-Aligned Manual Image Categorization, in Proceedings of 14th ACM Conference on Computer and Communications Security (CCS), Association for Computing Machinery, Inc., Oct. 2007\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "61\n",
      "Source text:\n",
      "COMET is an experiment being constructed at the J-PARC proton beam laboratory in Japan. It will search for coherent neutrino-less conversion of a muon to an electron, μ- + N(A,Z) → e- + N(A,Z). This process breaks the law of lepton conservation. If detected, it will be a signal of new physics. The previous upper limit for this decay was set [5] by the SINDRUM II experiment in 2006. COMET is designed to have 10,000 times better sensitivity. Cylindrical Drift Chamber The COMET experiment is looking for muon to electron conversion, μ- + N → e- + N. COMET Phase-I will the Cylindrical Drift Chamber as the primary detector for physics measurements. Specifically, the momentum of resulting particles will be measured using the CyDet, which is a cylindrical wire array detector. The particles flying out of muon-stopping target and registered by the CyDet. Among those we are interested in tracks left by electrons with specific energy, which are produced by muon to electron conversion. The CyDet consists of 4482 sensitive wires organized in 18 layers. Each wire measures the energy deposited by a passing charged particle. Within each of the layers, the wires have same distance to the stopping target and stereometry angle. There is magnetic field in the detector, which causes electron moves in helical path as shown below. This electron deposits energy in the wires close to the flight path. The radius of helix is proportional to transverse momentum of the electron: R = p_t/(eB) where p_t is transverse momentum, B is strength of magnetic field, e is charge of electron. The energy deposited on each wire is measured at the end plate of the cylindrical detector. An example of the resulting signal event can be seen below, where blue dots are background hits and red are hits from signal electrons: Starting Check out starter kit to make first submission and explore the data! More details COMET official site COMET conceptual design report Раритеты микромира - if you aren't deep into HEP, this article in russian is probably good starting point to understand what is COMET about. COMET presentation A search for μ-e conversion in muonic gold Important note Datasets available for this challenge are results of preliminary Monte Carlo simulation. They don't completely represent properties of COMET's detector and thus cannot be used to estimate final properties of tracking system, but are appropriate to test different approaches to tracking. Acknowledgements We thank COMET collaboration (and specially Chen WU) for allowing us to use this dataset.\n",
      "\n",
      "BERT summarization:\n",
      "COMET is an experiment being constructed at the J-PARC proton beam laboratory in Japan. Among those we are interested in tracks left by electrons with specific energy, which are produced by muon to electron conversion. Within each of the layers, the wires have same distance to the stopping target and stereometry angle. This electron deposits energy in the wires close to the flight path.\n",
      "\n",
      "BART summarization:\n",
      "COMET is an experiment being constructed at the J-PARC proton beam laboratory in Japan. It will search for coherent neutrino-less conversion of a muon to an electron. This process breaks the law of lepton conservation. If detected, it will be a signal of new physics.\n",
      "\n",
      "GPT2 summarization:\n",
      "COMET is an experiment being constructed at the J-PARC proton beam laboratory in Japan. It will search for coherent neutrino-less conversion of a muon to an electron, μ- + N(A,Z) → e- + N(A,Z). The energy deposited on each wire is measured at the end plate of the cylindrical detector. COMET presentation A search for μ-e conversion in muonic gold Important note Datasets available for this challenge are results of preliminary Monte Carlo simulation.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "110\n",
      "Source text:\n",
      "The goal of this task is to recover missing information about word casing and punctuation in English text. Such low quality text can be the result of automated speech transcription, or optical character recognition (OCR), or text written in a hurry such as quick notes in minutes, instant messaging, or web forums. To make this task easier, we have simplified it from a more ambitious task whose goal is to recover all the capitalisation and punctuation marks. For example, given the following text: ... stored at the ucla television archives the archived episodes were telecast march 8 16 and 24 1971 april 1 and ... We would hope to restore it to: ... stored at the UCLA Television Archives. The archived episodes were telecast: March 8, 16, and 24, 1971, April 1 and ... In this task we only ask you to predict whether a word in its original form has any characters in uppercase, and whether a word is followed by one of these punctuation marks: ,.;:?! You do not need to determine what particular characters of the word are in uppercase, or what punctuation mark follows the word. You will be given a file that lists a word per line like this: ID WORD 255 stored 256 at 257 the 258 ucla 259 television 260 archives 261 the 262 archived 263 episodes 264 were 265 telecast 266 march 267 8 268 16 269 and 270 24 271 1971 271 april 273 1 274 and The first line contains header information that you can ignore. Each of the following lines contains a word ID and the actual word. You will need to produce a file that lists the IDs of all words that have at least one capitalised character and the IDs of all words that are followed by a punctuation mark. The correct submission for the above example is: Id,documents Case,258 259 260 261 266 272 Punct,260 265 267 268 270 271 This submission says that word with ID 258 has at least one character in uppercase, word 260 has uppercase and punctuation marks, and so on. Key dates Release of training data: On registration. Deadline for submission of results over test data: 4 Oct 2013. Notification of results: 11 Oct 2013. Deadline for submission of system description poster: 27 Oct 2013. ALTA workshop : 5-6 December 2013. Further Information The usual approach to solve this type of task is to use Machine Learning methods. In the Data section of this competition we have made available an extract of Wikipedia that you can use to train your system, and the test data whose results you need to submit. We have also uploaded a sample Python program that uses NLTK and Hidden Markov Models to produce a solution. You can use it as your starting point, and as a baseline to beat. The results of this baseline are posted as the first submission of team \"Competition organisers\" in the leaderboard. For pointers to research on this area, visit: Useful information about the task at the ALTA website . Organisation This event is organised by the Australasian Language Technology Association ( ALTA ). For further details, or if you are interested in sponsoring this event, contact Diego Molla ( shared.task@alta.asn.au ).\n",
      "\n",
      "BERT summarization:\n",
      "The goal of this task is to recover missing information about word casing and punctuation in English text. To make this task easier, we have simplified it from a more ambitious task whose goal is to recover all the capitalisation and punctuation marks. The archived episodes were telecast: March 8, 16, and 24, 1971, April 1 and ... In this task we only ask you to predict whether a word in its original form has any characters in uppercase, and whether a word is followed by one of these punctuation marks: ,.;:?! Deadline for submission of system description poster: 27 Oct 2013. You can use it as your starting point, and as a baseline to beat.\n",
      "\n",
      "BART summarization:\n",
      "The goal of this task is to recover missing information about word casing and punctuation in English text. Such low quality text can be the result of automated speech transcription, or optical character recognition (OCR) To make this task easier, we have simplified it from a more ambitious task. In this task we only ask you to predict whether a word in its original form has any characters in uppercase.\n",
      "\n",
      "GPT2 summarization:\n",
      "The goal of this task is to recover missing information about word casing and punctuation in English text. The archived episodes were telecast: March 8, 16, and 24, 1971, April 1 and ... In this task we only ask you to predict whether a word in its original form has any characters in uppercase, and whether a word is followed by one of these punctuation marks: ,.;:?! Deadline for submission of results over test data: 4 Oct 2013. Organisation This event is organised by the Australasian Language Technology Association ( ALTA ).\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "115\n",
      "Source text:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. You have training dataset which is labelled & test dataset which is not labelled. You have to develop a classifier with the help of training data. Predict the labels of the test dataset with your classifier & submit it in a file with each label written in each row as 1 or 2. Format of the solution file you submit should be same as the file sample_submission.txt (i.e. 2 column with 1st column as ID & 2nd column as Label). You can submit either a .txt file or a .csv file. Here are few information about the data: No. of categories: 2 (represented as 1 or 2) No. of features: 54 Training data points: 383,701 Test data points: 197,311 Please make an account with your IIT Delhi university email id as you can participate with your IIT Delhi email-id account only. With each submission, please write a brief description of the model (e.g. Random Forest, 10 trees using 25 input variables etc). You can make 5 submissions each day until the 17th November 2013. There are 2 leaderboards - one public that is 30% of the test data and one private that is the other 70%. Public leaderboard will be used to evaluate your current score & ranking till deadline date. The final rankings will be based on the private leaderboard - so make sure you don't overfit your model to public leaderboard (have a look at http://blog.kaggle.com/2012/07/06/the-dangers-of-overfitting-psychopathy-post-mortem/). For any query or suggestion please mail me at rahulkumar.iitd57@gmail.com\n",
      "\n",
      "BERT summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. Format of the solution file you submit should be same as the file sample_submission.txt (i.e. 2 column with 1st column as ID & 2nd column as Label). of categories: 2 (represented as 1 or 2) No. You can make 5 submissions each day until the 17th November 2013.\n",
      "\n",
      "BART summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. You have to develop a classifier with the help of training data. There are 2 leaderboards - one public that is 30% of the test data and one private that is the other 70%. Public leaderboard will be used to evaluate your current score & ranking.\n",
      "\n",
      "GPT2 summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. With each submission, please write a brief description of the model (e.g. Random Forest, 10 trees using 25 input variables etc). Public leaderboard will be used to evaluate your current score & ranking till deadline date.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "117\n",
      "Source text:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. You have training dataset which is labelled & test dataset which is not labelled. You have to develop a classifier with the help of training data. Predict the labels of the test dataset with your classifier & submit it in a file with each label written in each row as +1 or -1. Format of the solution file you submit should be same as the file sample_submission.txt (i.e. 2 column with 1st column as ID & 2nd column as Label). You can submit either a .txt file or a .csv file. Here are few information about the data: No. of categories: 2 No. of features: 123 Training data points: 1,637 Test data points: 30,924 Please make an account with your IIT Delhi university email id as you can participate with your IIT Delhi email-id account only. With each submission, please write a brief description of the model (e.g. Random Forest, 10 trees using 25 input variables etc). You can make 5 submissions each day until the 17th November 2013. There are 2 leaderboards - one public that is 30% of the test data and one private that is the other 70%. Public leaderboard will be used to evaluate your current score & ranking till deadline date. The final rankings will be based on the private leaderboard - so make sure you don't overfit your model to public leaderboard (have a look at http://blog.kaggle.com/2012/07/06/the-dangers-of-overfitting-psychopathy-post-mortem/). For any query or suggestion please mail me at rahulkumar.iitd57@gmail.com\n",
      "\n",
      "BERT summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. Format of the solution file you submit should be same as the file sample_submission.txt (i.e. 2 column with 1st column as ID & 2nd column as Label). You can make 5 submissions each day until the 17th November 2013.\n",
      "\n",
      "BART summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. You have to develop a classifier with the help of training data. There are 2 leaderboards - one public that is 30% of the test data and one private that is the other 70%. Public leaderboard will be used to evaluate your current score & ranking till deadline date.\n",
      "\n",
      "GPT2 summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. of features: 123 Training data points: 1,637 Test data points: 30,924 Please make an account with your IIT Delhi university email id as you can participate with your IIT Delhi email-id account only. Public leaderboard will be used to evaluate your current score & ranking till deadline date.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "118\n",
      "Source text:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. You have training dataset which is labelled & test dataset which is not labelled. You have to develop a classifier with the help of training data. Predict the labels of the test dataset with your classifier & submit it in a file with each label written in each row as +1 or -1. Format of the solution file you submit should be same as the file sample_submission.txt (i.e. 2 column with 1st column as ID & 2nd column as Label). You can submit either a .txt file or a .csv file. Here are few information about the data: No. of categories: 2 No. of features: 123 Training data points: 32,103 Test data points: 16,739 Please make an account with your IIT Delhi university email id as you can participate with your IIT Delhi email-id account only. With each submission, please write a brief description of the model (e.g. Random Forest, 10 trees using 25 input variables etc). You can make 5 submissions each day until the 17th November 2013. There are 2 leaderboards - one public that is 30% of the test data and one private that is the other 70%. Public leaderboard will be used to evaluate your current score & ranking till deadline date. The final rankings will be based on the private leaderboard - so make sure you don't overfit your model to public leaderboard (have a look at http://blog.kaggle.com/2012/07/06/the-dangers-of-overfitting-psychopathy-post-mortem/). For any query or suggestion please mail me at rahulkumar.iitd57@gmail.com\n",
      "\n",
      "BERT summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. Format of the solution file you submit should be same as the file sample_submission.txt (i.e. 2 column with 1st column as ID & 2nd column as Label). You can make 5 submissions each day until the 17th November 2013.\n",
      "\n",
      "BART summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. You have to develop a classifier with the help of training data. There are 2 leaderboards - one public that is 30% of the test data and one private that is the other 70%. Public leaderboard will be used to evaluate your current score & ranking till deadline date.\n",
      "\n",
      "GPT2 summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. of features: 123 Training data points: 32,103 Test data points: 16,739 Please make an account with your IIT Delhi university email id as you can participate with your IIT Delhi email-id account only. Public leaderboard will be used to evaluate your current score & ranking till deadline date.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "119\n",
      "Source text:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. You have training dataset which is labelled & test dataset which is not labelled. You have to develop a classifier with the help of training data. Predict the labels of the test dataset with your classifier & submit it in a file with each label written in each row as 1 or -1. Format of the solution file you submit should be same as the file sample_submission.txt (i.e. 2 column with 1st column as ID & 2nd column as Label). You can submit either a .txt file or a .csv file. Here are few information about the data: No. of categories: 2 No. of features: 60 Training data points: 148 Test data points: 60 Please make an account with your IIT Delhi university email id as you can participate with your IIT Delhi email-id account only. With each submission, please write a brief description of the model (e.g. Random Forest, 10 trees using 25 input variables etc). You can make 5 submissions each day until the 17th November 2013. There are 2 leaderboards - one public that is 30% of the test data and one private that is the other 70%. Public leaderboard will be used to evaluate your current score & ranking till deadline date. The final rankings will be based on the private leaderboard - so make sure you don't overfit your model to public leaderboard (have a look at http://blog.kaggle.com/2012/07/06/the-dangers-of-overfitting-psychopathy-post-mortem/). For any query or suggestion please mail me at rahulkumar.iitd57@gmail.com\n",
      "\n",
      "BERT summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. Format of the solution file you submit should be same as the file sample_submission.txt (i.e. 2 column with 1st column as ID & 2nd column as Label). You can make 5 submissions each day until the 17th November 2013.\n",
      "\n",
      "BART summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. You have to develop a classifier with the help of training data. There are 2 leaderboards - one public that is 30% of the test data and one private that is the other 70%. Public leaderboard will be used to evaluate your current score & ranking.\n",
      "\n",
      "GPT2 summarization:\n",
      "This is the Machine Learning competition for the course CSV884 at IIT Delhi. With each submission, please write a brief description of the model (e.g. Random Forest, 10 trees using 25 input variables etc). Public leaderboard will be used to evaluate your current score & ranking till deadline date.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "146\n",
      "Source text:\n",
      "Understanding the brain structure and some of its disease alterations is key to research on the treatment of epilepsy, Alzheimer's disease, and other neuropathologies, as well as understanding the general function of the brain and its learning capabilities. The brain contains nearly 100 billion neurons with an average 7000 synaptic connections.  Recovering the exact wiring of the brain (connectome) at this neural level is therefore a daunting task. Traditional neuroanatomic methods of axonal tracing cannot scale up to very large networks. Could there be alternative methods to recovering neural network structures from patterns of neural activity? [ Learn more ... ] What you get: Time series of the activity of 1000 neurons. What you predict: The directed connections between neurons. Get started: (1) 5 min tutorial on brain sciences (2) Starter kit Today's cutting edge optical imaging of neural activity (using fluorescent calcium indicators) provides a tool to monitor the activity of tens of thousands of neurons simultaneously. Mathematical algorithms capable of discovering network structures are faced with the challenge of solving a new inverse problem: recover the neural network structure of a living system given the observation of a very large population of neurons. A promising way to experimentally proceed is to use neuronal cultures . Such cultures consist in a number of individual cells (dissected and dissociated from actual brain tissue) that are plated on a cover glass and maintained for several weeks in vitro . These living neuronal networks typically contain on the order of few thousand cells. One can then monitor their activity by fluorescence imaging, reconstruct their connectivity from activity data and, finally, compare the reconstructed circuitry with the real one. However, to fully understand the degree of accuracy of the reconstruction one needs first to procure superior reconstruction algorithms: this is where you can help by entering this competition! Monitoring changes in effective connectivity patterns of a network during behavior promises to advance our understanding of learning and intelligence. This challenge will stimulate research on network-structure learning from neurophysiological data, including causal discovery methods. [ Learn more... ] Brain of the zebrafish in action. Today's cutting edge neurophysiology multi-electrode recording tools are capable of  recording (and even stimulating) of the order of 100 neurons. Optical imaging of neural activity using fluorescent calcium indicator molecules (calcium imaging) increases the number of neurons recorded by three orders of magnitude. Recently, researchers have been able to record in vivo the activity of the brain of a zebrafish embryo in 80% of its 100,000 neurons. This video comes from the work of Arens et al. Nature  485,  471–477 (May 2012) . Acknowledgements This competition is brought to you by ChaLearn . See our credits page .\n",
      "\n",
      "BERT summarization:\n",
      "Understanding the brain structure and some of its disease alterations is key to research on the treatment of epilepsy, Alzheimer's disease, and other neuropathologies, as well as understanding the general function of the brain and its learning capabilities. Could there be alternative methods to recovering neural network structures from patterns of neural activity? [ Learn more ... ] What you get: Time series of the activity of 1000 neurons. Get started: (1) 5 min tutorial on brain sciences (2) Starter kit Today's cutting edge optical imaging of neural activity (using fluorescent calcium indicators) provides a tool to monitor the activity of tens of thousands of neurons simultaneously. Such cultures consist in a number of individual cells (dissected and dissociated from actual brain tissue) that are plated on a cover glass and maintained for several weeks in vitro .\n",
      "\n",
      "BART summarization:\n",
      "The brain contains nearly 100 billion neurons with an average 7000 synaptic connections. Traditional neuroanatomic methods of axonal tracing cannot scale up to very large networks. Could there be alternative methods to recovering neural network structures from patterns of neural activity? What you get: Time series of the activity of 1000 neurons. What you predict: The directed connections between neurons.\n",
      "\n",
      "GPT2 summarization:\n",
      "Understanding the brain structure and some of its disease alterations is key to research on the treatment of epilepsy, Alzheimer's disease, and other neuropathologies, as well as understanding the general function of the brain and its learning capabilities. Learn more ... ] What you get: Time series of the activity of 1000 neurons. These living neuronal networks typically contain on the order of few thousand cells. This challenge will stimulate research on network-structure learning from neurophysiological data, including causal discovery methods. [\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "228\n",
      "Source text:\n",
      "Image source: Weisstein, Eric W. \" Game of Life. \" From MathWorld--A Wolfram Web Resource. The Game of Life is a cellular automaton created by mathematician John Conway in 1970. The game consists of a board of cells that are either on or off. One creates an initial configuration of these on/off states and observes how it evolves. There are four simple rules to determine the next state of the game board, given the current state: Any live cell with fewer than two live neighbors dies, as if caused by under-population. Any live cell with two or three live neighbors lives on to the next generation. Any live cell with more than three live neighbors dies, as if by overcrowding. Any dead cell with exactly three live neighbors becomes a live cell, as if by reproduction. These simple rules result in many interesting behaviors and have been the focus of a large body of mathematics.  As Wikipedia tells it, Ever since its publication, Conway's Game of Life has attracted much interest, because of the surprising ways in which the patterns can evolve. Life provides an example of emergence and self-organization. It is interesting for computer scientists, physicists, biologists, biochemists, economists, mathematicians, philosophers, generative scientists and others to observe the way that complex patterns can emerge from the implementation of very simple rules. The game can also serve as a didactic analogy, used to convey the somewhat counter-intuitive notion that \"design\" and \"organization\" can spontaneously emerge in the absence of a designer. For example, philosopher and cognitive scientist Daniel Dennett has used the analogue of Conway's Life \"universe\" extensively to illustrate the possible evolution of complex philosophical constructs, such as consciousness and free will, from the relatively simple set of deterministic physical laws governing our own universe. The emergence of order from simple rules begs an interesting question-- what happens if we set time backwards? This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse.  Is the chaotic start of Life predictable from its orderly ends?  We have created many games, evolved them, and provided only the end boards. You are asked to predict the starting board that resulted in each end board. Although some people have examined this problem, it is unknown (at least, to us...) just how difficult this will be.\n",
      "\n",
      "BERT summarization:\n",
      "Image source: Weisstein, Eric W. \" Game of Life. \" Any live cell with two or three live neighbors lives on to the next generation. The game can also serve as a didactic analogy, used to convey the somewhat counter-intuitive notion that \"design\" and \"organization\" can spontaneously emerge in the absence of a designer. This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse. Is the chaotic start of Life predictable from its orderly ends?\n",
      "\n",
      "BART summarization:\n",
      "The Game of Life is a cellular automaton created by mathematician John Conway in 1970. Life provides an example of emergence and self-organization. The emergence of order from simple rules begs an interesting question-- what happens if we set time backwards? This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse.\n",
      "\n",
      "GPT2 summarization:\n",
      "The Game of Life is a cellular automaton created by mathematician John Conway in 1970. Any live cell with more than three live neighbors dies, as if by overcrowding. It is interesting for computer scientists, physicists, biologists, biochemists, economists, mathematicians, philosophers, generative scientists and others to observe the way that complex patterns can emerge from the implementation of very simple rules. This competition is an experiment to see if machine learning (or optimization, or any method) can predict the game of life in reverse.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Text id:\n",
      "268\n",
      "Source text:\n",
      "The majority of tools for bacteria detection and recognition are based on physiological or genetic properties of microorganisms. However, there is an enormous interest in devising truly label-free and reagentless biosensors which would operate utilizing the biophysical nature of the samples without the need for sensing and reporting biochemistry. The phenotypic biophysical sensors are closest to realizing this goal. Whether they are based on mass spectroscopy or on forward scatter phenotyping, phenotypic biosensors rely on a library of spectral signatures generated for different bacterial classes to subsequently detect and classify future samples of unknown nature. In this competition participants will use unsupervised feature learning and supervised classification techniques to classify bacteria from four different genuses (e.coli, listeria, salmonella, staphylococus). What makes this problem challenging are: No hand-crafted feature extraction is allowed. All features should be derived in an unsupervised manner starting with the individual pixels. Training, public, and private data sets were collected during a 3-day time frame.  We speculate that data collected on different days could have been impacted by random effects. Making sure that a classifier trained on day 1 data set generalizes to day 2 and 3 data sets is the main goal of this competition. Data Set You will be provided about 1800 images. This image set is split into three as train, public, and private. You will be provided with labels for images in the training set (Train_images.zip) and build a classifier to predict labels for images in the public and private sets. Public and private images are provided in the Test_images.zip file. You won't know which images are public and which are private. When you submit your predictions your public and private scores will be separately computed but you will only get to see your public score. Acknowledgements We thank Dr. Bartek Rajwa, Ph.D of Purdue University Bindley Bioscience Center for providing the bacteria dataset.\n",
      "\n",
      "BERT summarization:\n",
      "The majority of tools for bacteria detection and recognition are based on physiological or genetic properties of microorganisms. Making sure that a classifier trained on day 1 data set generalizes to day 2 and 3 data sets is the main goal of this competition. You will be provided with labels for images in the training set (Train_images.zip) and build a classifier to predict labels for images in the public and private sets. When you submit your predictions your public and private scores will be separately computed but you will only get to see your public score.\n",
      "\n",
      "BART summarization:\n",
      "The majority of tools for bacteria detection and recognition are based on physiological or genetic properties of microorganisms. There is an enormous interest in devising truly label-free and reagentless biosensors. In this competition participants will use unsupervised feature learning and supervised classification techniques to classify bacteria from four different genuses.\n",
      "\n",
      "GPT2 summarization:\n",
      "The majority of tools for bacteria detection and recognition are based on physiological or genetic properties of microorganisms. Whether they are based on mass spectroscopy or on forward scatter phenotyping, phenotypic biosensors rely on a library of spectral signatures generated for different bacterial classes to subsequently detect and classify future samples of unknown nature. Making sure that a classifier trained on day 1 data set generalizes to day 2 and 3 data sets is the main goal of this competition. Public and private images are provided in the Test_images.zip file.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(x[1]) for x in texts_20_30[:15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
